# ChatXFELé¡¹ç›®ä¼˜åŒ– - ä¸‰å‘¨å¼€å‘è®¡åˆ’

## é¡¹ç›®æ¦‚è¿°

**ç›®æ ‡**: ä¼˜åŒ–ChatXFELçš„æ£€ç´¢è¿‡ç¨‹ï¼Œæé«˜å›ç­”è´¨é‡

**æ ¸å¿ƒä»»åŠ¡**ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰:
1. æ·»åŠ Rewriteå’ŒRerankæ¨¡å—ï¼ˆé«˜çº§RAGæŠ€æœ¯ï¼‰
2. å­åº“è·¯ç”±ï¼šå…ˆæ£€ç´¢æ‘˜è¦ï¼Œå†æ£€ç´¢å¯¹åº”çš„å…¨æ–‡
3. å®ç°å…ƒæ•°æ®è¿‡æ»¤ï¼Œæ”¯æŒæŒ‰å…³é”®è¯è¿‡æ»¤æ–‡çŒ®
4. æ··åˆæ£€ç´¢ï¼šåŒæ—¶ä½¿ç”¨denseå’Œsparseå‘é‡

**ç¯å¢ƒä¿¡æ¯**:
- å¤§æ¨¡å‹ï¼šqwen3:30b-a3b-instruct-2507-q8_0 (http://10.15.102.186:9000)
- å‘é‡æ¨¡å‹ï¼šBGE-M3 (æ”¯æŒdense+sparseåŒå‘é‡)
- å‘é‡æ•°æ®åº“ï¼šMilvus 2.5.22 (10.19.48.181:19530)
- å¼€å‘æ¡†æ¶ï¼šLangChain

---

## ç¬¬ä¸€å‘¨ï¼šç¯å¢ƒæ­å»º + Rewrite & Rerankå¢å¼º

### Checkpoint 1.1: ç¯å¢ƒé…ç½®ï¼ˆDay 1-2ï¼‰

#### ä»»åŠ¡æ¸…å•
- [ ] é…ç½®Milvus 2.5.22æ•°æ®åº“è¿æ¥
  - æ•°æ®åº“åœ°å€ï¼š10.19.48.181:19530
  - ç”¨æˆ·åï¼šcs286_2025_groupX (X=5æˆ–8)
  - å¯†ç ï¼šGroupX
  - æ•°æ®åº“åï¼šcs286_2025_groupX
  - Attuå¯è§†åŒ–ç•Œé¢ï¼š10.19.48.181:30411

- [ ] éƒ¨ç½²/è¿æ¥Qwen3å¤§æ¨¡å‹
  - è®¿é—®åœ°å€ï¼šhttp://10.15.102.186:9000
  - æ¨¡å‹åï¼šqwen3:30b-a3b-instruct-2507-q8_0
  - é€šè¿‡ollamaè®¿é—®

- [ ] éƒ¨ç½²BGE-M3å‘é‡åŒ–æ¨¡å‹
  - æ¨¡å‹åï¼šbge-m3:latest
  - ç‰¹æ€§ï¼šåŒæ—¶ç”Ÿæˆdenseå’Œsparseå‘é‡
  - é€šè¿‡ollamaè®¿é—®

- [ ] æµ‹è¯•Rerankeræ¨¡å‹
  - é€‰é¡¹1ï¼šBGE-Reranker-v2-m3
  - é€‰é¡¹2ï¼šQwen3-Rerankerç³»åˆ—
  - æ¨èä½¿ç”¨BGE-Reranker-v2-m3ï¼ˆä¸ç°æœ‰ä»£ç å…¼å®¹ï¼‰

- [ ] éªŒè¯LangChainç¯å¢ƒ
  - æ£€æŸ¥ä¾èµ–åŒ…ç‰ˆæœ¬
  - æµ‹è¯•ç°æœ‰ä»£ç ï¼ˆchatxfel_app.py, rag.pyï¼‰
  - ç¡®ä¿ä¸æ–°ç‰ˆæœ¬LangChainå…¼å®¹

#### éªŒæ”¶æ ‡å‡†
- æˆåŠŸè¿æ¥æ‰€æœ‰æœåŠ¡
- èƒ½å¤Ÿè°ƒç”¨æ¨¡å‹è¿›è¡Œç®€å•çš„é—®ç­”æµ‹è¯•
- ç°æœ‰ä»£ç èƒ½æ­£å¸¸è¿è¡Œ

---

### Checkpoint 1.2: Query Rewriteæ¨¡å—ï¼ˆDay 3-5ï¼‰

#### å®ç°ç­–ç•¥

**ç­–ç•¥1ï¼šQueryæ‰©å±•**
```python
# ä½¿ç”¨LLMæ‰©å±•queryï¼Œæ·»åŠ åŒä¹‰è¯å’Œä¸“ä¸šæœ¯è¯­
prompt = """
è¯·å¯¹ä»¥ä¸‹XFELé¢†åŸŸçš„é—®é¢˜è¿›è¡Œæ‰©å±•ï¼Œæ·»åŠ ç›¸å…³çš„åŒä¹‰è¯å’Œä¸“ä¸šæœ¯è¯­ï¼š
é—®é¢˜ï¼š{original_query}
æ‰©å±•åçš„é—®é¢˜ï¼š
"""
```

**ç­–ç•¥2ï¼šQueryåˆ†è§£**
```python
# å°†å¤æ‚é—®é¢˜æ‹†åˆ†æˆå¤šä¸ªå­é—®é¢˜
prompt = """
è¯·å°†ä»¥ä¸‹å¤æ‚é—®é¢˜åˆ†è§£ä¸º2-3ä¸ªæ›´ç®€å•çš„å­é—®é¢˜ï¼š
é—®é¢˜ï¼š{original_query}
å­é—®é¢˜ï¼š
1. 
2. 
3. 
"""
```

**ç­–ç•¥3ï¼šå›è¯‘å¢å¼ºï¼ˆHyDEï¼‰**
```python
# è®©LLMå…ˆç”Ÿæˆå‡è®¾æ€§ç­”æ¡ˆï¼Œç”¨ç­”æ¡ˆè¿›è¡Œæ£€ç´¢
prompt = """
è¯·å¯¹ä»¥ä¸‹é—®é¢˜ç”Ÿæˆä¸€ä¸ªå‡è®¾æ€§çš„ç­”æ¡ˆï¼š
é—®é¢˜ï¼š{original_query}
å‡è®¾ç­”æ¡ˆï¼š
"""
```

#### ä»»åŠ¡æ¸…å•
- [ ] åœ¨`rag.py`ä¸­æ·»åŠ `query_rewrite()`å‡½æ•°
- [ ] å®ç°è‡³å°‘2ç§rewriteç­–ç•¥
- [ ] è®¾è®¡A/Bæµ‹è¯•ï¼šå¯¹æ¯”åŸå§‹query vs. rewritten query
- [ ] æµ‹è¯•10ä¸ªé—®é¢˜ï¼Œè®°å½•æ£€ç´¢ç»“æœå·®å¼‚
- [ ] é›†æˆåˆ°ç°æœ‰RAG pipeline

#### ä»£ç ç¤ºä¾‹
```python
def query_rewrite(query: str, llm, strategy: str = 'expand') -> str:
    """
    é‡å†™ç”¨æˆ·æŸ¥è¯¢
    
    Args:
        query: åŸå§‹æŸ¥è¯¢
        llm: å¤§è¯­è¨€æ¨¡å‹
        strategy: 'expand', 'decompose', 'hyde'
    
    Returns:
        é‡å†™åçš„æŸ¥è¯¢
    """
    if strategy == 'expand':
        # å®ç°queryæ‰©å±•
        pass
    elif strategy == 'decompose':
        # å®ç°queryåˆ†è§£
        pass
    elif strategy == 'hyde':
        # å®ç°HyDE
        pass
    return rewritten_query
```

#### éªŒæ”¶æ ‡å‡†
- å®Œæˆè‡³å°‘2ç§rewriteç­–ç•¥
- æœ‰å¯¹æ¯”å®éªŒæ•°æ®
- èƒ½å¤Ÿæå‡æ£€ç´¢ç›¸å…³æ€§

---

### Checkpoint 1.3: Rerankä¼˜åŒ–ï¼ˆDay 6-7ï¼‰

#### ä»»åŠ¡æ¸…å•
- [ ] åˆ†æç°æœ‰rerankä»£ç ï¼ˆ`chatxfel_app.py`ä¸­å·²æœ‰ï¼‰
- [ ] ä¼˜åŒ–å‚æ•°ï¼š
  - `top_k`ï¼šåˆæ£€æ•°é‡ï¼ˆå»ºè®®10-20ï¼‰
  - `top_n`ï¼šrerankåä¿ç•™æ•°é‡ï¼ˆå»ºè®®5-8ï¼‰
- [ ] å®ç°two-stage rerankingï¼š
  - Stage 1: å¿«é€Ÿç²—æ’ï¼ˆä½¿ç”¨ç®€å•æ¨¡å‹æˆ–è§„åˆ™ï¼‰
  - Stage 2: ç²¾å‡†ç²¾æ’ï¼ˆä½¿ç”¨BGE-Rerankerï¼‰
- [ ] æ€§èƒ½æµ‹è¯•ï¼šè®°å½•rerankå‰åçš„ç›¸å…³æ€§å¾—åˆ†

#### ä»£ç ä¼˜åŒ–ç‚¹
```python
# ç°æœ‰ä»£ç ä¸­çš„rerank
compressor = get_rerank_model(top_n=n_recall)
compression_retriever = ContextualCompressionRetriever(
    base_compressor=compressor,
    base_retriever=retriever_obj.as_retriever(search_kwargs=search_kwargs)
)

# ä¼˜åŒ–å»ºè®®ï¼šå¢åŠ two-stage
def two_stage_rerank(docs, query, top_k=20, top_n=6):
    # Stage 1: ç²—æ’ï¼ˆå¿«é€Ÿè¿‡æ»¤ï¼‰
    stage1_docs = coarse_rank(docs, query, top_k=top_k)
    
    # Stage 2: ç²¾æ’ï¼ˆç²¾å‡†æ’åºï¼‰
    stage2_docs = fine_rank(stage1_docs, query, top_n=top_n)
    
    return stage2_docs
```

#### éªŒæ”¶æ ‡å‡†
- Reranké€Ÿåº¦æå‡æˆ–æ•ˆæœæå‡
- æœ‰è¯¦ç»†çš„å‚æ•°è°ƒä¼˜è®°å½•

---

## ç¬¬äºŒå‘¨ï¼šæ··åˆæ£€ç´¢ + å­åº“è·¯ç”±

### Checkpoint 2.1: æ··åˆæ£€ç´¢å®ç°ï¼ˆDay 8-10ï¼‰

#### æŠ€æœ¯åŸç†
BGE-M3æ¨¡å‹å¯ä»¥åŒæ—¶ç”Ÿæˆï¼š
- **Denseå‘é‡**ï¼šæ•æ‰è¯­ä¹‰ç›¸ä¼¼æ€§
- **Sparseå‘é‡**ï¼šæ•æ‰å…³é”®è¯åŒ¹é…

æ··åˆæ£€ç´¢å…¬å¼ï¼š`score = Î± * dense_score + (1-Î±) * sparse_score`

#### ä»»åŠ¡æ¸…å•

**Step 1: ä¿®æ”¹å‘é‡åŒ–ä»£ç **
- [ ] æ›´æ–°`vectorize_bibs.py`
- [ ] ç¡®ä¿åŒæ—¶å­˜å‚¨denseå’Œsparseå‘é‡
- [ ] éªŒè¯ç°æœ‰æ•°æ®æ˜¯å¦å·²æœ‰åŒå‘é‡

```python
# æ£€æŸ¥ç°æœ‰collection schema
from pymilvus import Collection, connections

connections.connect(**connection_args)
collection = Collection(name="your_collection")
print(collection.schema)  # æ£€æŸ¥æ˜¯å¦æœ‰sparse_vectorå­—æ®µ
```

**Step 2: åˆ›å»ºæ··åˆæ£€ç´¢collection**
- [ ] åœ¨Milvusä¸­åˆ›å»ºæ–°çš„collection
- [ ] SchemaåŒ…å«ï¼š
  - dense_vector (FLOAT_VECTOR, dim=1024)
  - sparse_vector (SPARSE_FLOAT_VECTOR)
  - å…ƒæ•°æ®å­—æ®µï¼ˆtitle, doi, journal, yearç­‰ï¼‰

```python
# å‚è€ƒvectorize_bibs.pyä¸­çš„create_bge_collection_by_connection()
fields = [
    FieldSchema(name='dense_vector', dtype=DataType.FLOAT_VECTOR, dim=1024),
    FieldSchema(name='sparse_vector', dtype=DataType.SPARSE_FLOAT_VECTOR),
    # ... å…¶ä»–å­—æ®µ
]
```

**Step 3: å®ç°æ··åˆæ£€ç´¢**
- [ ] åœ¨`rag.py`ä¸­æ·»åŠ `hybrid_search()`å‡½æ•°
- [ ] å®ç°åŠ æƒç­–ç•¥ï¼ˆdenseæƒé‡å¯è°ƒï¼‰
- [ ] é›†æˆåˆ°retriever

```python
def hybrid_search(query, collection, embedding, dense_weight=0.7, top_k=10):
    """
    æ··åˆæ£€ç´¢ï¼šdense + sparse
    
    Args:
        query: æŸ¥è¯¢æ–‡æœ¬
        collection: Milvus collection
        embedding: BGE-M3æ¨¡å‹
        dense_weight: denseå‘é‡æƒé‡ (0-1)
        top_k: è¿”å›ç»“æœæ•°é‡
    
    Returns:
        æ£€ç´¢ç»“æœ
    """
    # ç”Ÿæˆqueryçš„åŒå‘é‡
    query_vectors = embedding.encode_queries([query])
    dense_vec = query_vectors['dense'][0]
    sparse_vec = query_vectors['sparse'][0]
    
    # Milvusæ··åˆæ£€ç´¢
    search_params = {
        "data": [[dense_vec], [sparse_vec]],
        "anns_field": ["dense_vector", "sparse_vector"],
        "param": [
            {"metric_type": "IP", "params": {"nprobe": 10}},
            {"metric_type": "IP", "params": {}}
        ],
        "limit": top_k,
        "weights": [dense_weight, 1-dense_weight]
    }
    
    results = collection.hybrid_search(**search_params)
    return results
```

**Step 4: å¯¹æ¯”å®éªŒ**
- [ ] è®¾è®¡å¯¹æ¯”å®éªŒï¼š
  - çº¯denseæ£€ç´¢
  - çº¯sparseæ£€ç´¢
  - æ··åˆæ£€ç´¢ï¼ˆä¸åŒæƒé‡ï¼‰
- [ ] æµ‹è¯•20ä¸ªé—®é¢˜
- [ ] è®°å½•ç»“æœå¹¶åˆ†æ

#### éªŒæ”¶æ ‡å‡†
- æˆåŠŸå®ç°æ··åˆæ£€ç´¢
- æœ‰å¯¹æ¯”å®éªŒæ•°æ®
- æ‰¾åˆ°æœ€ä¼˜çš„dense_weightå‚æ•°

---

### Checkpoint 2.2: å­åº“è·¯ç”±ç³»ç»Ÿï¼ˆDay 11-14ï¼‰

#### ç³»ç»Ÿæ¶æ„

```
User Query
    â†“
[Abstract Collection] â† ç¬¬ä¸€æ­¥ï¼šæ£€ç´¢ç›¸å…³è®ºæ–‡æ‘˜è¦
    â†“ (è·å–DOI/Title)
[Fulltext Collection]  â† ç¬¬äºŒæ­¥ï¼šæ£€ç´¢å¯¹åº”çš„å…¨æ–‡chunks
    â†“
Generate Answer
```

#### ä»»åŠ¡æ¸…å•

**Phase 1: æ•°æ®å‡†å¤‡ï¼ˆDay 11ï¼‰**
- [ ] åˆ†æMongoDBä¸­çš„è®ºæ–‡æ•°æ®ç»“æ„
- [ ] æå–æ‰€æœ‰è®ºæ–‡çš„æ‘˜è¦ï¼ˆabstractå­—æ®µï¼‰
- [ ] æå–æ‰€æœ‰è®ºæ–‡çš„å…¨æ–‡chunksï¼ˆå·²æœ‰çš„splitç»“æœï¼‰

```python
# ä»MongoDBæå–æ‘˜è¦
def extract_abstracts(mongo_collection):
    """æå–æ‰€æœ‰è®ºæ–‡æ‘˜è¦"""
    docs = mongo_collection.find(
        filter={'abstract': {'$ne': ''}},
        projection={'title': 1, 'doi': 1, 'abstract': 1, 'year': 1, 'journal': 1}
    )
    return list(docs)
```

**Phase 2: åˆ›å»ºæ‘˜è¦åº“ï¼ˆDay 11-12ï¼‰**
- [ ] åˆ›å»ºabstract_collection
- [ ] Schemaè®¾è®¡ï¼š
  ```python
  fields = [
      FieldSchema(name='title', dtype=DataType.VARCHAR, max_length=1000),
      FieldSchema(name='doi', dtype=DataType.VARCHAR, max_length=1000, is_primary_key=True),
      FieldSchema(name='abstract', dtype=DataType.VARCHAR, max_length=10000),
      FieldSchema(name='year', dtype=DataType.INT16),
      FieldSchema(name='journal', dtype=DataType.VARCHAR, max_length=500),
      FieldSchema(name='dense_vector', dtype=DataType.FLOAT_VECTOR, dim=1024),
      FieldSchema(name='sparse_vector', dtype=DataType.SPARSE_FLOAT_VECTOR),
  ]
  ```
- [ ] å‘é‡åŒ–æ‰€æœ‰æ‘˜è¦å¹¶æ’å…¥

**Phase 3: åˆ›å»ºå…¨æ–‡åº“ï¼ˆDay 12ï¼‰**
- [ ] åˆ›å»ºfulltext_collectionï¼ˆå¯èƒ½å·²å­˜åœ¨ï¼‰
- [ ] ç¡®ä¿æ¯ä¸ªchunkéƒ½å…³è”åˆ°DOI/Title
- [ ] æ·»åŠ ç´¢å¼•ä»¥æ”¯æŒå¿«é€Ÿè¿‡æ»¤

**Phase 4: å®ç°è·¯ç”±é€»è¾‘ï¼ˆDay 13-14ï¼‰**
- [ ] åœ¨`rag.py`ä¸­æ·»åŠ `route_retrieval()`å‡½æ•°

```python
def route_retrieval(query, abstract_collection, fulltext_collection, 
                   embedding, top_papers=5, top_chunks=10):
    """
    ä¸¤é˜¶æ®µæ£€ç´¢ï¼šå…ˆæ‘˜è¦åå…¨æ–‡
    
    Args:
        query: ç”¨æˆ·æŸ¥è¯¢
        abstract_collection: æ‘˜è¦åº“
        fulltext_collection: å…¨æ–‡åº“
        embedding: å‘é‡åŒ–æ¨¡å‹
        top_papers: ä»æ‘˜è¦åº“æ£€ç´¢çš„è®ºæ–‡æ•°
        top_chunks: ä»æ¯ç¯‡è®ºæ–‡æ£€ç´¢çš„chunkæ•°
    
    Returns:
        æœ€ç›¸å…³çš„æ–‡æœ¬chunks
    """
    # Step 1: åœ¨æ‘˜è¦åº“ä¸­æ£€ç´¢
    relevant_papers = hybrid_search(
        query=query,
        collection=abstract_collection,
        embedding=embedding,
        top_k=top_papers
    )
    
    # Step 2: è·å–ç›¸å…³è®ºæ–‡çš„DOIåˆ—è¡¨
    dois = [paper['doi'] for paper in relevant_papers]
    
    # Step 3: åœ¨å…¨æ–‡åº“ä¸­è¿‡æ»¤æ£€ç´¢
    # åªåœ¨è¿™äº›DOIå¯¹åº”çš„chunksä¸­æœç´¢
    filter_expr = f"doi in {dois}"
    fulltext_results = hybrid_search(
        query=query,
        collection=fulltext_collection,
        embedding=embedding,
        top_k=top_chunks,
        filter=filter_expr
    )
    
    return fulltext_results
```

- [ ] é›†æˆåˆ°ä¸»pipeline
- [ ] æ·»åŠ fallbackæœºåˆ¶ï¼šå¦‚æœæ‘˜è¦åº“æœªæ‰¾åˆ°ï¼Œç›´æ¥æœå…¨æ–‡

**Phase 5: æµ‹è¯•ä¸ä¼˜åŒ–ï¼ˆDay 14ï¼‰**
- [ ] å¯¹æ¯”å®éªŒï¼š
  - ç›´æ¥å…¨æ–‡æ£€ç´¢
  - å­åº“è·¯ç”±æ£€ç´¢
- [ ] è°ƒä¼˜å‚æ•°ï¼š
  - top_papersï¼ˆå»ºè®®3-5ï¼‰
  - top_chunksï¼ˆå»ºè®®æ¯ç¯‡è®ºæ–‡2-3ä¸ªchunksï¼‰
- [ ] åˆ†ææ£€ç´¢é€Ÿåº¦å’Œå‡†ç¡®æ€§

#### éªŒæ”¶æ ‡å‡†
- æˆåŠŸå®ç°ä¸¤é˜¶æ®µæ£€ç´¢
- æ£€ç´¢ç²¾åº¦æœ‰æå‡
- æ£€ç´¢é€Ÿåº¦å¯æ¥å—ï¼ˆå»ºè®®<3ç§’ï¼‰

---

## ç¬¬ä¸‰å‘¨ï¼šå…ƒæ•°æ®è¿‡æ»¤ + Deep Research Agentï¼ˆåŠ åˆ†é¡¹ï¼‰+ è¯„ä¼°ä¼˜åŒ–

### Checkpoint 3.1: å…ƒæ•°æ®è¿‡æ»¤ç³»ç»Ÿï¼ˆDay 15-16ï¼‰

#### åŠŸèƒ½éœ€æ±‚
æ”¯æŒç”¨æˆ·é€šè¿‡ä»¥ä¸‹ç»´åº¦è¿‡æ»¤æ–‡çŒ®ï¼š
1. **å¹´ä»½èŒƒå›´**ï¼ˆå·²æœ‰ï¼Œéœ€ä¼˜åŒ–ï¼‰
2. **æœŸåˆŠåç§°**
3. **å…³é”®è¯**ï¼ˆæ ‡é¢˜æˆ–æ‘˜è¦ä¸­åŒ…å«ï¼‰
4. **ç ”ç©¶æœºæ„/è£…ç½®**ï¼ˆfacilityå­—æ®µï¼‰

#### ä»»åŠ¡æ¸…å•

**Task 1: æ‰©å±•Milvus Schemaï¼ˆDay 15ï¼‰**
- [ ] æ£€æŸ¥ç°æœ‰schemaï¼Œç¡®è®¤æ‰€æœ‰éœ€è¦çš„å…ƒæ•°æ®å­—æ®µ
- [ ] å¦‚éœ€æ·»åŠ æ–°å­—æ®µï¼ˆå¦‚keywordsï¼‰ï¼Œæ›´æ–°schema
- [ ] å¯èƒ½éœ€è¦é‡æ–°å‘é‡åŒ–éƒ¨åˆ†æ•°æ®

```python
# æ·»åŠ keywordså­—æ®µ
schema.add_field(
    field_name='keywords',
    datatype=DataType.VARCHAR,
    max_length=500
)
```

**Task 2: å®ç°åŠ¨æ€è¿‡æ»¤ï¼ˆDay 15ï¼‰**
- [ ] åœ¨`rag.py`ä¸­æ·»åŠ `build_filter_expression()`å‡½æ•°

```python
def build_filter_expression(filters: dict) -> str:
    """
    æ„å»ºMilvusè¿‡æ»¤è¡¨è¾¾å¼
    
    Args:
        filters: {
            'year_range': (2018, 2024),
            'journals': ['Nature', 'Science'],
            'keywords': ['SFX', 'crystallography'],
            'facility': 'LCLS'
        }
    
    Returns:
        Milvus filter expression
    """
    expressions = []
    
    if 'year_range' in filters:
        start, end = filters['year_range']
        expressions.append(f"{start} <= year <= {end}")
    
    if 'journals' in filters:
        journals = filters['journals']
        journal_expr = " or ".join([f'journal == "{j}"' for j in journals])
        expressions.append(f"({journal_expr})")
    
    if 'keywords' in filters:
        keywords = filters['keywords']
        # æ³¨æ„ï¼šMilvusçš„å­—ç¬¦ä¸²åŒ¹é…è¯­æ³•
        keyword_expr = " or ".join([f'title like "%{kw}%"' for kw in keywords])
        expressions.append(f"({keyword_expr})")
    
    if 'facility' in filters:
        expressions.append(f'facility == "{filters["facility"]}"')
    
    return " and ".join(expressions)
```

**Task 3: æ›´æ–°UIï¼ˆDay 16ï¼‰**
- [ ] åœ¨`chatxfel_app.py`ä¸­æ·»åŠ è¿‡æ»¤é€‰é¡¹
- [ ] ä¼˜åŒ–ç°æœ‰çš„`filter_year`åŠŸèƒ½
- [ ] æ·»åŠ æ–°çš„è¿‡æ»¤æ§ä»¶

```python
# åœ¨sidebarä¸­æ·»åŠ 
with st.sidebar:
    # å¹´ä»½è¿‡æ»¤ï¼ˆå·²æœ‰ï¼Œä¿ç•™ï¼‰
    filter_year = st.checkbox('Filter by year', value=True)
    if filter_year:
        year_start = st.selectbox('Start year', ...)
        year_end = st.selectbox('End year', ...)
    
    # æœŸåˆŠè¿‡æ»¤ï¼ˆæ–°å¢ï¼‰
    filter_journal = st.checkbox('Filter by journal', value=False)
    if filter_journal:
        journals = st.multiselect(
            'Select journals',
            options=['Nature', 'Science', 'Physical Review', ...]
        )
    
    # å…³é”®è¯è¿‡æ»¤ï¼ˆæ–°å¢ï¼‰
    filter_keywords = st.checkbox('Filter by keywords', value=False)
    if filter_keywords:
        keywords_input = st.text_input(
            'Keywords (comma separated)',
            placeholder='SFX, crystallography, XFEL'
        )
        keywords = [kw.strip() for kw in keywords_input.split(',') if kw.strip()]
    
    # è£…ç½®è¿‡æ»¤ï¼ˆæ–°å¢ï¼‰
    filter_facility = st.checkbox('Filter by facility', value=False)
    if filter_facility:
        facility = st.selectbox(
            'Select facility',
            options=['LCLS', 'EuXFEL', 'SACLA', 'PAL-XFEL', ...]
        )
```

**Task 4: é›†æˆåˆ°æ£€ç´¢æµç¨‹ï¼ˆDay 16ï¼‰**
- [ ] å°†è¿‡æ»¤æ¡ä»¶ä¼ é€’ç»™retriever
- [ ] æµ‹è¯•å„ç§è¿‡æ»¤ç»„åˆ
- [ ] ç¡®ä¿è¿‡æ»¤ä¸å½±å“æ£€ç´¢é€Ÿåº¦

#### éªŒæ”¶æ ‡å‡†
- æ”¯æŒè‡³å°‘3ç§è¿‡æ»¤ç»´åº¦
- UIäº¤äº’æµç•…
- è¿‡æ»¤åŠŸèƒ½æ­£ç¡®

---

### Checkpoint 3.1+: Deep Research Agentï¼ˆDay 17-18ï¼ŒåŠ åˆ†é¡¹ï¼‰

> **é‡è¦è¯´æ˜**ï¼šæ­¤åŠŸèƒ½ä¸º**åŠ åˆ†é¡¹**ï¼Œåº”åœ¨ç¡®ä¿æ ¸å¿ƒåŠŸèƒ½ï¼ˆRewriteã€Rerankã€æ··åˆæ£€ç´¢ã€å­åº“è·¯ç”±ï¼‰éƒ½å®Œæˆåå†å®æ–½ã€‚å¦‚æœæ—¶é—´ç´§å¼ ï¼Œå¯ä»¥å®ç°ç®€åŒ–ç‰ˆæœ¬æˆ–è·³è¿‡æ­¤éƒ¨åˆ†ã€‚

#### åŠŸèƒ½ç›®æ ‡

å®ç°ä¸€ä¸ªåŸºäºReActçš„Agentç³»ç»Ÿï¼Œèƒ½å¤Ÿï¼š
1. **è‡ªåŠ¨åˆ†è§£é—®é¢˜**ï¼šå°†å¤æ‚é—®é¢˜æ‹†è§£æˆå­é—®é¢˜
2. **å¤šè½®è¿­ä»£æ£€ç´¢**ï¼šæ ¹æ®ä¸­é—´ç»“æœå†³å®šä¸‹ä¸€æ­¥æ£€ç´¢ç­–ç•¥
3. **ç»¼åˆä¿¡æ¯**ï¼šæ•´åˆå¤šä¸ªæ¥æºçš„ä¿¡æ¯ç”Ÿæˆæ·±åº¦æŠ¥å‘Š
4. **å±•ç¤ºæ€ç»´è¿‡ç¨‹**ï¼šè®©ç”¨æˆ·çœ‹åˆ°Agentçš„æ¨ç†è¿‡ç¨‹

**ä½¿ç”¨åœºæ™¯ç¤ºä¾‹**ï¼š
- ç”¨æˆ·é—®ï¼š"æ€»ç»“XFELåœ¨è›‹ç™½è´¨ç»“æ„è§£æä¸­çš„åº”ç”¨å’Œæœ€æ–°è¿›å±•"
- Agentä¼šï¼š
  - Step 1: å…ˆæ£€ç´¢"XFEL protein structure"çš„åŸºç¡€çŸ¥è¯†
  - Step 2: æ£€ç´¢"SFX crystallography recent advances"
  - Step 3: æ£€ç´¢å…·ä½“çš„åº”ç”¨æ¡ˆä¾‹
  - Step 4: ç»¼åˆæ‰€æœ‰ä¿¡æ¯ç”Ÿæˆç»¼è¿°æŠ¥å‘Š

#### ReActæ¡†æ¶åŸç†

```
å¾ªç¯ç›´åˆ°å®Œæˆï¼š
    Thought: æˆ‘ç°åœ¨éœ€è¦äº†è§£ä»€ä¹ˆï¼Ÿ
    Action: ä½¿ç”¨å“ªä¸ªå·¥å…·ï¼Ÿ(search/retrieve/summarize)
    Observation: å·¥å…·è¿”å›äº†ä»€ä¹ˆç»“æœï¼Ÿ
    [å¦‚æœä¿¡æ¯å……åˆ†] â†’ Final Answer
    [å¦‚æœä¿¡æ¯ä¸è¶³] â†’ ç»§ç»­å¾ªç¯
```

#### ä»»åŠ¡æ¸…å•

**Phase 1: è®¾è®¡Agent Toolsï¼ˆDay 17ä¸Šåˆï¼‰**

å®šä¹‰Agentå¯ä»¥ä½¿ç”¨çš„å·¥å…·ï¼š

- [ ] **Tool 1: search_papers**
  ```python
  def search_papers(query: str, filters: dict = None) -> List[str]:
      """
      æœç´¢ç›¸å…³è®ºæ–‡æ ‡é¢˜å’Œæ‘˜è¦
      
      Args:
          query: æœç´¢æŸ¥è¯¢
          filters: è¿‡æ»¤æ¡ä»¶ï¼ˆå¹´ä»½ã€æœŸåˆŠç­‰ï¼‰
      
      Returns:
          è®ºæ–‡åˆ—è¡¨ï¼ˆæ ‡é¢˜+æ‘˜è¦ç‰‡æ®µï¼‰
      """
      # ä½¿ç”¨abstract_collectionæ£€ç´¢
      pass
  ```

- [ ] **Tool 2: retrieve_details**
  ```python
  def retrieve_details(paper_titles: List[str], aspect: str) -> str:
      """
      è·å–ç‰¹å®šè®ºæ–‡çš„è¯¦ç»†å†…å®¹
      
      Args:
          paper_titles: è®ºæ–‡æ ‡é¢˜åˆ—è¡¨
          aspect: å…³æ³¨çš„æ–¹é¢ï¼ˆå¦‚"methods", "results", "applications"ï¼‰
      
      Returns:
          ç›¸å…³çš„è¯¦ç»†å†…å®¹
      """
      # ä½¿ç”¨fulltext_collectionæ£€ç´¢ç‰¹å®šè®ºæ–‡çš„chunks
      pass
  ```

- [ ] **Tool 3: summarize_findings**
  ```python
  def summarize_findings(documents: List[str]) -> str:
      """
      æ€»ç»“å½“å‰å·²æ£€ç´¢åˆ°çš„æ–‡çŒ®
      
      Args:
          documents: æ–‡çŒ®å†…å®¹åˆ—è¡¨
      
      Returns:
          æ‘˜è¦æ–‡æœ¬
      """
      # ä½¿ç”¨LLMæ€»ç»“
      pass
  ```

**Phase 2: å®ç°ReAct Agentï¼ˆDay 17ä¸‹åˆï¼‰**

- [ ] ä½¿ç”¨LangChainçš„ReActæ¡†æ¶

```python
from langchain.agents import create_react_agent, AgentExecutor
from langchain.tools import Tool
from langchain.prompts import PromptTemplate

# å®šä¹‰ReActæç¤ºè¯
REACT_PROMPT = """You are a research assistant specialized in XFEL (X-ray Free Electron Laser) literature.
Your goal is to thoroughly research the user's question by iteratively searching and analyzing papers.

You have access to the following tools:

{tools}

Use the following format:

Question: the input question you must answer
Thought: think about what information you need
Action: the action to take, should be one of [{tool_names}]
Action Input: the input to the action
Observation: the result of the action
... (this Thought/Action/Action Input/Observation can repeat N times)
Thought: I now have enough information to answer
Final Answer: the final comprehensive answer to the original question

IMPORTANT GUIDELINES:
1. Break down complex questions into sub-questions
2. Search for 2-3 different aspects of the question
3. After each search, analyze if you have enough information
4. Aim for 3-5 iterations before giving final answer
5. Cite specific papers in your final answer

Begin!

Question: {input}
Thought: {agent_scratchpad}"""

def create_research_agent(llm, tools):
    """åˆ›å»ºReActç ”ç©¶Agent"""
    
    prompt = PromptTemplate.from_template(REACT_PROMPT)
    
    agent = create_react_agent(
        llm=llm,
        tools=tools,
        prompt=prompt
    )
    
    agent_executor = AgentExecutor(
        agent=agent,
        tools=tools,
        verbose=True,  # æ˜¾ç¤ºæ€è€ƒè¿‡ç¨‹
        max_iterations=6,  # æœ€å¤š6è½®è¿­ä»£
        max_execution_time=120,  # æœ€å¤š2åˆ†é’Ÿ
        handle_parsing_errors=True
    )
    
    return agent_executor
```

- [ ] é›†æˆtoolsåˆ°Agent

```python
# åˆ›å»ºå·¥å…·åˆ—è¡¨
tools = [
    Tool(
        name="search_papers",
        func=search_papers,
        description="Useful for finding relevant papers. Input should be a search query string."
    ),
    Tool(
        name="retrieve_details",
        func=lambda x: retrieve_details(x.split(','), aspect="methods"),
        description="Useful for getting detailed content from specific papers. Input should be comma-separated paper titles."
    ),
    Tool(
        name="summarize_findings",
        func=summarize_findings,
        description="Useful for summarizing current research findings. Input should be 'summarize'."
    )
]

# åˆ›å»ºagent
research_agent = create_research_agent(llm, tools)
```

**Phase 3: å®ç°å¤šè½®å¯¹è¯å’ŒçŠ¶æ€ç®¡ç†ï¼ˆDay 18ä¸Šåˆï¼‰**

- [ ] æ·»åŠ å¯¹è¯å†å²è®°å½•
- [ ] å®ç°ä¸­é—´ç»“æœç¼“å­˜

```python
class ResearchSession:
    """ç®¡ç†ä¸€æ¬¡Deep Researchä¼šè¯"""
    
    def __init__(self, agent_executor):
        self.agent = agent_executor
        self.history = []
        self.intermediate_results = {}
    
    def research(self, question: str) -> dict:
        """
        æ‰§è¡Œæ·±åº¦ç ”ç©¶
        
        Returns:
            {
                'answer': æœ€ç»ˆç­”æ¡ˆ,
                'reasoning_steps': æ¨ç†æ­¥éª¤åˆ—è¡¨,
                'papers_used': ä½¿ç”¨çš„è®ºæ–‡åˆ—è¡¨,
                'iterations': è¿­ä»£æ¬¡æ•°
            }
        """
        # è®°å½•æ¨ç†è¿‡ç¨‹
        reasoning_steps = []
        
        # æ‰§è¡ŒAgent
        result = self.agent.invoke(
            {"input": question},
            callbacks=[ReasoningCallback(reasoning_steps)]
        )
        
        # æå–ä½¿ç”¨çš„è®ºæ–‡
        papers_used = self._extract_papers(reasoning_steps)
        
        return {
            'answer': result['output'],
            'reasoning_steps': reasoning_steps,
            'papers_used': papers_used,
            'iterations': len(reasoning_steps)
        }
    
    def _extract_papers(self, steps):
        """ä»æ¨ç†æ­¥éª¤ä¸­æå–å¼•ç”¨çš„è®ºæ–‡"""
        papers = []
        for step in steps:
            if 'Observation' in step:
                # è§£æå‡ºè®ºæ–‡æ ‡é¢˜
                pass
        return papers
```

**Phase 4: UIé›†æˆï¼ˆDay 18ä¸‹åˆï¼‰**

- [ ] åœ¨Streamlitä¸­æ·»åŠ "Deep Research"æ¨¡å¼

```python
# åœ¨chatxfel_app.pyä¸­æ·»åŠ 
with st.sidebar:
    research_mode = st.radio(
        "Mode",
        options=["Quick Answer", "Deep Research"],
        help="Quick Answer: å•æ¬¡æ£€ç´¢\nDeep Research: å¤šè½®è¿­ä»£ç ”ç©¶ï¼ˆè¾ƒæ…¢ï¼‰"
    )
    
    if research_mode == "Deep Research":
        max_iterations = st.slider("Max iterations", 3, 8, 5)
        show_reasoning = st.checkbox("Show reasoning process", value=True)

# åœ¨ä¸»æµç¨‹ä¸­
if research_mode == "Deep Research":
    with st.spinner("ğŸ”¬ Conducting deep research..."):
        session = ResearchSession(research_agent)
        result = session.research(question)
        
        # æ˜¾ç¤ºæ¨ç†è¿‡ç¨‹
        if show_reasoning:
            with st.expander("ğŸ§  Reasoning Process"):
                for i, step in enumerate(result['reasoning_steps']):
                    st.markdown(f"**Step {i+1}**")
                    st.text(step['thought'])
                    st.info(f"Action: {step['action']}")
                    st.success(f"Observation: {step['observation'][:200]}...")
        
        # æ˜¾ç¤ºæœ€ç»ˆç­”æ¡ˆ
        st.markdown("### ğŸ“Š Research Report")
        st.markdown(result['answer'])
        
        # æ˜¾ç¤ºå¼•ç”¨çš„è®ºæ–‡
        with st.expander("ğŸ“š Papers Referenced"):
            for paper in result['papers_used']:
                st.markdown(f"- {paper}")
```

#### ç®€åŒ–æ–¹æ¡ˆï¼ˆå¦‚æœæ—¶é—´ç´§å¼ ï¼‰

å¦‚æœæ—¶é—´ä¸å¤Ÿï¼Œå¯ä»¥å®ç°**æœ€ç®€ç‰ˆæœ¬**ï¼š

**æ ¸å¿ƒåŠŸèƒ½**ï¼š
1. âœ… åªå®ç°2ä¸ªå·¥å…·ï¼šsearch + summarize
2. âœ… å›ºå®š3è½®è¿­ä»£ï¼ˆä¸éœ€è¦å¤æ‚çš„åœæ­¢æ¡ä»¶ï¼‰
3. âœ… åœ¨UIä¸­æ·»åŠ ä¸€ä¸ª"Deep Research"æŒ‰é’®

**ä»£ç ç¤ºä¾‹**ï¼ˆç®€åŒ–ç‰ˆï¼‰ï¼š
```python
def simple_deep_research(question: str, llm, retriever) -> dict:
    """
    ç®€åŒ–çš„Deep Researchï¼šå›ºå®š3è½®è¿­ä»£
    """
    results = []
    
    # Round 1: åˆæ­¥æ£€ç´¢
    thought_1 = f"First, I need to understand the basics of: {question}"
    docs_1 = retriever.get_relevant_documents(question)
    summary_1 = llm.invoke(f"Summarize these papers: {docs_1[:3]}")
    results.append({
        'round': 1,
        'thought': thought_1,
        'action': 'search',
        'docs': docs_1[:3],
        'summary': summary_1
    })
    
    # Round 2: æ·±å…¥ç‰¹å®šæ–¹é¢
    thought_2 = "Now I need more specific information about applications"
    refined_query = f"{question} applications methods"
    docs_2 = retriever.get_relevant_documents(refined_query)
    summary_2 = llm.invoke(f"Summarize these papers: {docs_2[:3]}")
    results.append({
        'round': 2,
        'thought': thought_2,
        'action': 'search_specific',
        'docs': docs_2[:3],
        'summary': summary_2
    })
    
    # Round 3: ç»¼åˆç­”æ¡ˆ
    thought_3 = "Now I can synthesize all information"
    final_answer = llm.invoke(f"""Based on the following research:
    Round 1: {summary_1}
    Round 2: {summary_2}
    
    Please provide a comprehensive answer to: {question}
    """)
    results.append({
        'round': 3,
        'thought': thought_3,
        'action': 'synthesize',
        'answer': final_answer
    })
    
    return {
        'answer': final_answer,
        'steps': results
    }
```

#### æµ‹è¯•ç”¨ä¾‹

è®¾è®¡3ä¸ªæµ‹è¯•é—®é¢˜ï¼ˆä»ç®€å•åˆ°å¤æ‚ï¼‰ï¼š

1. **ç®€å•é—®é¢˜**ï¼š
   - "What is serial femtosecond crystallography?"
   - é¢„æœŸï¼š2-3è½®è¿­ä»£å³å¯

2. **ä¸­ç­‰é—®é¢˜**ï¼š
   - "Compare the data processing pipelines used at LCLS and EuXFEL"
   - é¢„æœŸï¼š4-5è½®è¿­ä»£

3. **å¤æ‚é—®é¢˜**ï¼š
   - "Summarize the evolution of XFEL technology from 2010 to 2024, focusing on improvements in pulse duration, repetition rate, and scientific applications"
   - é¢„æœŸï¼š5-6è½®è¿­ä»£

#### éªŒæ”¶æ ‡å‡†

**å¿…é¡»å®Œæˆ**ï¼ˆç®€åŒ–ç‰ˆï¼‰ï¼š
- [ ] å®ç°å›ºå®š3è½®è¿­ä»£çš„simple_deep_research
- [ ] åœ¨UIä¸­æ·»åŠ "Deep Research"æ¨¡å¼
- [ ] èƒ½å¤Ÿå±•ç¤ºæ¨ç†è¿‡ç¨‹
- [ ] æµ‹è¯•è‡³å°‘1ä¸ªé—®é¢˜

**åŠ åˆ†å®Œæˆ**ï¼ˆå®Œæ•´ç‰ˆï¼‰ï¼š
- [ ] å®ç°å®Œæ•´çš„ReAct Agent
- [ ] æ”¯æŒåŠ¨æ€è¿­ä»£æ¬¡æ•°
- [ ] æœ‰è¯¦ç»†çš„reasoningå±•ç¤º
- [ ] æµ‹è¯•3ä¸ªä¸åŒå¤æ‚åº¦çš„é—®é¢˜

#### æ—¶é—´ç®¡ç†å»ºè®®

**ç­–ç•¥Aï¼ˆä¿å®ˆï¼‰**ï¼š
- Day 17: å¦‚æœå‰é¢è¿›åº¦æ­£å¸¸ï¼Œå¼€å§‹å®ç°ç®€åŒ–ç‰ˆ
- Day 18: å®Œå–„å’Œæµ‹è¯•
- å¦‚æœæ—¶é—´ä¸å¤Ÿï¼Œ**æ”¾å¼ƒæ­¤åŠŸèƒ½**ï¼Œä¸“æ³¨æ ¸å¿ƒåŠŸèƒ½

**ç­–ç•¥Bï¼ˆæ¿€è¿›ï¼‰**ï¼š
- Day 17: å®ç°å®Œæ•´ReAct Agent
- Day 18: UIé›†æˆå’Œæµ‹è¯•
- å¯èƒ½éœ€è¦ç‰ºç‰²éƒ¨åˆ†è¯„ä¼°æ—¶é—´

**æ¨èé‡‡ç”¨ç­–ç•¥A**ï¼Œç¡®ä¿æ ¸å¿ƒåŠŸèƒ½ç¨³å®šã€‚

---

### Checkpoint 3.2: ç³»ç»Ÿé›†æˆä¸æµ‹è¯•ï¼ˆDay 19ï¼‰

#### å®Œæ•´Pipeline

```
User Query
    â†“
[Query Rewrite] â† æŸ¥è¯¢æ”¹å†™ï¼ˆæ‰©å±•/åˆ†è§£ï¼‰
    â†“
[Metadata Filter] â† åº”ç”¨ç”¨æˆ·è®¾å®šçš„è¿‡æ»¤æ¡ä»¶
    â†“
[Abstract Retrieval] â† åœ¨æ‘˜è¦åº“ä¸­æ£€ç´¢ï¼ˆæ··åˆæ£€ç´¢ï¼‰
    â†“
[Route to Fulltext] â† è·¯ç”±åˆ°å…¨æ–‡åº“
    â†“
[Fulltext Retrieval] â† åœ¨å…¨æ–‡chunksä¸­æ£€ç´¢ï¼ˆæ··åˆæ£€ç´¢ï¼‰
    â†“
[Rerank] â† Two-stageé‡æ’åº
    â†“
[Generate Answer] â† LLMç”Ÿæˆç­”æ¡ˆ
    â†“
Response to User
```

#### ä»»åŠ¡æ¸…å•

**Day 19: æ¨¡å—æ•´åˆä¸æµ‹è¯•**
- [ ] åˆ›å»ºæ–°çš„ä¸»å‡½æ•°`advanced_rag_pipeline()`
- [ ] é›†æˆæ‰€æœ‰æ¨¡å—ï¼š
  - query_rewrite
  - build_filter_expression
  - route_retrieval
  - hybrid_search
  - two_stage_rerank
- [ ] æ·»åŠ é”™è¯¯å¤„ç†å’Œæ—¥å¿—

```python
def advanced_rag_pipeline(query: str, 
                         llm, 
                         embedding,
                         abstract_collection,
                         fulltext_collection,
                         filters: dict = None,
                         use_rewrite: bool = True,
                         use_routing: bool = True) -> dict:
    """
    é«˜çº§RAG pipeline
    
    Returns:
        {
            'answer': str,
            'context': List[Document],
            'metadata': {
                'rewritten_query': str,
                'papers_found': int,
                'retrieval_time': float
            }
        }
    """
    import time
    start_time = time.time()
    
    # Step 1: Query Rewrite
    if use_rewrite:
        rewritten_query = query_rewrite(query, llm, strategy='expand')
    else:
        rewritten_query = query
    
    # Step 2: Build Filter
    filter_expr = build_filter_expression(filters) if filters else None
    
    # Step 3: Retrieval
    if use_routing:
        # ä¸¤é˜¶æ®µæ£€ç´¢
        docs = route_retrieval(
            query=rewritten_query,
            abstract_collection=abstract_collection,
            fulltext_collection=fulltext_collection,
            embedding=embedding,
            filter=filter_expr
        )
    else:
        # ç›´æ¥å…¨æ–‡æ£€ç´¢
        docs = hybrid_search(
            query=rewritten_query,
            collection=fulltext_collection,
            embedding=embedding,
            filter=filter_expr
        )
    
    # Step 4: Rerank
    ranked_docs = two_stage_rerank(docs, rewritten_query, top_n=6)
    
    # Step 5: Generate
    answer = generate_answer(query, ranked_docs, llm)
    
    retrieval_time = time.time() - start_time
    
    return {
        'answer': answer,
        'context': ranked_docs,
        'metadata': {
            'rewritten_query': rewritten_query,
            'papers_found': len(ranked_docs),
            'retrieval_time': retrieval_time
        }
    }
```

- [ ] å•å…ƒæµ‹è¯•ï¼šæµ‹è¯•æ¯ä¸ªæ¨¡å—
- [ ] é›†æˆæµ‹è¯•ï¼šæµ‹è¯•å®Œæ•´pipeline
- [ ] æ€§èƒ½æµ‹è¯•ï¼š
  - å“åº”æ—¶é—´ï¼ˆç›®æ ‡<5ç§’ï¼‰
  - å¹¶å‘èƒ½åŠ›
  - èµ„æºå ç”¨
- [ ] å‹åŠ›æµ‹è¯•ï¼šè¿ç»­100æ¬¡æŸ¥è¯¢

#### éªŒæ”¶æ ‡å‡†
- æ‰€æœ‰æ¨¡å—æ­£å¸¸å·¥ä½œ
- Pipelineç¨³å®šè¿è¡Œ
- æœ‰å®Œæ•´çš„æµ‹è¯•æŠ¥å‘Š

---

### Checkpoint 3.3: è¯„ä¼°ä¸æŠ¥å‘Šï¼ˆDay 20-21ï¼‰

#### è¯„ä¼°ä»»åŠ¡1ï¼šæ–‡çŒ®åº“å¯¹æ¯”å®éªŒ

**ç›®æ ‡**ï¼šéªŒè¯åœ¨å¤§æ–‡çŒ®åº“ä¸­çš„æ£€ç´¢ä¸€è‡´æ€§

**å®éªŒè®¾è®¡**ï¼š
- [ ] æ„å»ºæµ‹è¯•é›†Aï¼šç²¾é€‰100ç¯‡é«˜è´¨é‡è®ºæ–‡
- [ ] æ„å»ºæµ‹è¯•é›†Bï¼šA + 900ç¯‡å…¶ä»–è®ºæ–‡ï¼ˆå…±1000ç¯‡ï¼‰
- [ ] è®¾è®¡10ä¸ªæ ‡å‡†æµ‹è¯•é—®é¢˜ï¼š
  ```
  1. What is serial femtosecond crystallography?
  2. How does XFEL compare to synchrotron radiation?
  3. What are the main data processing challenges in SFX?
  4. Describe the pump-probe technique in XFEL experiments.
  5. What is the typical pulse duration of XFEL?
  6. How to prepare samples for SPI experiments?
  7. What are the advantages of EuXFEL over LCLS?
  8. Explain the concept of hit-finding in XFEL data.
  9. What software tools are used for XFEL data analysis?
  10. What are recent developments in XFEL technology?
  ```

**è¯„ä¼°æŒ‡æ ‡**ï¼š
- [ ] Top-5æ–‡çŒ®é‡å ç‡ï¼š`overlap = len(set(A_docs) & set(B_docs)) / 5`
- [ ] æ–‡çŒ®æ’åºç›¸å…³æ€§ï¼šKendall's Ï„
- [ ] ç­”æ¡ˆBLEUå¾—åˆ†ï¼ˆå¦‚æœAå’ŒBçš„ç­”æ¡ˆåº”è¯¥ç›¸ä¼¼ï¼‰

**å®éªŒæ­¥éª¤**ï¼š
```python
def evaluate_consistency(questions, collection_A, collection_B, pipeline):
    """è¯„ä¼°åœ¨ä¸åŒå¤§å°æ–‡çŒ®åº“ä¸­çš„ä¸€è‡´æ€§"""
    results = []
    
    for q in questions:
        # åœ¨Aä¸­æ£€ç´¢
        docs_A = pipeline(q, collection=collection_A)
        
        # åœ¨Bä¸­æ£€ç´¢
        docs_B = pipeline(q, collection=collection_B)
        
        # è®¡ç®—é‡å ç‡
        overlap = calculate_overlap(docs_A, docs_B, top_k=5)
        
        results.append({
            'question': q,
            'overlap_rate': overlap,
            'docs_A': docs_A,
            'docs_B': docs_B
        })
    
    return results
```

---

#### è¯„ä¼°ä»»åŠ¡2ï¼šå›ç­”è´¨é‡è¯„ä¼°

**ç›®æ ‡**ï¼šè¯„ä¼°å›ç­”çš„å‡†ç¡®æ€§å’Œå‚è€ƒæ–‡çŒ®çš„ç›¸å…³æ€§

**æµ‹è¯•é›†**ï¼š
- [ ] å‡†å¤‡20ä¸ªæµ‹è¯•é—®é¢˜ï¼ˆè¦†ç›–ä¸åŒéš¾åº¦ï¼‰
  - ç®€å•äº‹å®æ€§é—®é¢˜ï¼ˆ5ä¸ªï¼‰
  - ä¸­ç­‰å¤æ‚åº¦é—®é¢˜ï¼ˆ10ä¸ªï¼‰
  - å¤æ‚ç»¼åˆæ€§é—®é¢˜ï¼ˆ5ä¸ªï¼‰

**è¯„ä¼°ç»´åº¦**ï¼š
1. **ç­”æ¡ˆå‡†ç¡®æ€§**ï¼ˆäººå·¥è¯„åˆ†ï¼Œ1-5åˆ†ï¼‰
   - 5åˆ†ï¼šå®Œå…¨å‡†ç¡®ï¼Œè¯¦ç»†å®Œæ•´
   - 4åˆ†ï¼šåŸºæœ¬å‡†ç¡®ï¼Œæœ‰å°‘é‡é—æ¼
   - 3åˆ†ï¼šéƒ¨åˆ†å‡†ç¡®ï¼Œæœ‰é”™è¯¯
   - 2åˆ†ï¼šå¤§éƒ¨åˆ†é”™è¯¯
   - 1åˆ†ï¼šå®Œå…¨é”™è¯¯

2. **å‚è€ƒæ–‡çŒ®ç›¸å…³æ€§**ï¼ˆäººå·¥è¯„åˆ†ï¼Œ1-5åˆ†ï¼‰
   - 5åˆ†ï¼šæ‰€æœ‰æ–‡çŒ®é«˜åº¦ç›¸å…³
   - 4åˆ†ï¼šå¤šæ•°æ–‡çŒ®ç›¸å…³
   - 3åˆ†ï¼šéƒ¨åˆ†æ–‡çŒ®ç›¸å…³
   - 2åˆ†ï¼šå°‘æ•°æ–‡çŒ®ç›¸å…³
   - 1åˆ†ï¼šæ–‡çŒ®ä¸ç›¸å…³

3. **å“åº”æ—¶é—´**ï¼ˆè‡ªåŠ¨è®°å½•ï¼‰
   - ç›®æ ‡ï¼š<5ç§’

**å¯¹æ¯”å®éªŒ**ï¼š
- [ ] Baselineï¼ˆä¼˜åŒ–å‰ï¼‰ï¼šç°æœ‰ç³»ç»Ÿ
- [ ] System-1ï¼š+ Rewrite + Rerank
- [ ] System-2ï¼š+ Hybrid Search
- [ ] System-3ï¼š+ Routing
- [ ] System-4ï¼ˆå®Œæ•´ç‰ˆï¼‰ï¼šæ‰€æœ‰ä¼˜åŒ–

**è¯„ä¼°ä»£ç **ï¼š
```python
def evaluate_qa_quality(questions, systems):
    """è¯„ä¼°é—®ç­”è´¨é‡"""
    results = {sys_name: [] for sys_name in systems.keys()}
    
    for q in questions:
        for sys_name, sys_pipeline in systems.items():
            start_time = time.time()
            
            response = sys_pipeline(q)
            
            response_time = time.time() - start_time
            
            # è®°å½•ç»“æœï¼ˆäººå·¥è¯„åˆ†åå¡«å…¥ï¼‰
            results[sys_name].append({
                'question': q,
                'answer': response['answer'],
                'sources': response['context'],
                'response_time': response_time,
                'accuracy_score': None,  # å¾…äººå·¥è¯„åˆ†
                'relevance_score': None  # å¾…äººå·¥è¯„åˆ†
            })
    
    return results
```

---

#### æ’°å†™é¡¹ç›®æŠ¥å‘Šï¼ˆDay 21ï¼‰

**æŠ¥å‘Šç»“æ„**ï¼š

```markdown
# ChatXFELç³»ç»Ÿä¼˜åŒ–æŠ¥å‘Š

## 1. é¡¹ç›®æ¦‚è¿°
- 1.1 èƒŒæ™¯ä¸ç›®æ ‡
- 1.2 ä¼˜åŒ–ä»»åŠ¡
- 1.3 æŠ€æœ¯æ ˆ

## 2. æŠ€æœ¯å®ç°

### 2.1 Query Rewriteæ¨¡å—
- å®ç°çš„ç­–ç•¥
- ä»£ç ç¤ºä¾‹
- æ•ˆæœå¯¹æ¯”

### 2.2 Hybrid Searchï¼ˆæ··åˆæ£€ç´¢ï¼‰
- Dense + SparseåŒå‘é‡
- æƒé‡è°ƒä¼˜
- æ€§èƒ½æå‡

### 2.3 å­åº“è·¯ç”±ç³»ç»Ÿ
- æ¶æ„è®¾è®¡
- å®ç°ç»†èŠ‚
- æ£€ç´¢åŠ é€Ÿæ•ˆæœ

### 2.4 Rerankä¼˜åŒ–
- Two-stageç­–ç•¥
- å‚æ•°è°ƒä¼˜
- ç›¸å…³æ€§æå‡

### 2.5 å…ƒæ•°æ®è¿‡æ»¤
- æ”¯æŒçš„è¿‡æ»¤ç»´åº¦
- UIè®¾è®¡
- ä½¿ç”¨ç¤ºä¾‹

### 2.6 Deep Research Agentï¼ˆåŠ åˆ†é¡¹ï¼‰
- ReActæ¡†æ¶å®ç°
- å·¥å…·è®¾è®¡
- å¤šè½®è¿­ä»£ç­–ç•¥
- æ€ç»´è¿‡ç¨‹å±•ç¤º

## 3. å®éªŒç»“æœ

### 3.1 ä¸€è‡´æ€§æµ‹è¯•
- å®éªŒè®¾è®¡
- æ•°æ®ç»Ÿè®¡ï¼ˆè¡¨æ ¼+å›¾è¡¨ï¼‰
- ç»“æœåˆ†æ

### 3.2 è´¨é‡è¯„ä¼°
- è¯„åˆ†ç»Ÿè®¡
- ç³»ç»Ÿå¯¹æ¯”ï¼ˆæŸ±çŠ¶å›¾ï¼‰
- Case studyï¼ˆå±•ç¤º2-3ä¸ªå…¸å‹æ¡ˆä¾‹ï¼‰

### 3.3 æ€§èƒ½æµ‹è¯•
- å“åº”æ—¶é—´å¯¹æ¯”
- èµ„æºå ç”¨
- å¹¶å‘èƒ½åŠ›

### 3.4 Deep Research Agentè¯„ä¼°ï¼ˆå¦‚å·²å®ç°ï¼‰
- è¿­ä»£æ¬¡æ•°åˆ†æ
- ä¿¡æ¯è¦†ç›–åº¦
- ç­”æ¡ˆæ·±åº¦å¯¹æ¯”ï¼ˆvs. Quick Answeræ¨¡å¼ï¼‰
- Case studyå±•ç¤º

## 4. é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ

### 4.1 é‡åˆ°çš„ä¸»è¦é—®é¢˜
- é—®é¢˜1: Milvus 2.5å…¼å®¹æ€§
  - è§£å†³æ–¹æ¡ˆ: ...
  
- é—®é¢˜2: å‘é‡åŒ–é€Ÿåº¦æ…¢
  - è§£å†³æ–¹æ¡ˆ: ...

### 4.2 å¾…æ”¹è¿›ä¹‹å¤„
- åŠŸèƒ½å±‚é¢
- æ€§èƒ½å±‚é¢
- ç”¨æˆ·ä½“éªŒ

## 5. æ€»ç»“ä¸å±•æœ›

### 5.1 å®Œæˆæƒ…å†µ
- å·²å®Œæˆçš„åŠŸèƒ½
- è¾¾æˆçš„ç›®æ ‡

### 5.2 æœªæ¥å·¥ä½œ
- Short-termï¼ˆ3ä¸ªæœˆå†…ï¼‰
- Long-termï¼ˆ6ä¸ªæœˆä»¥ä¸Šï¼‰

## 6. é™„å½•
- ä»£ç ä»“åº“é“¾æ¥
- æ•°æ®é›†è¯´æ˜
- å®Œæ•´æµ‹è¯•ç»“æœ
```

**é…å›¾è¦æ±‚**ï¼š
- [ ] ç³»ç»Ÿæ¶æ„å›¾ï¼ˆä½¿ç”¨draw.ioæˆ–mermaidï¼‰
- [ ] Pipelineæµç¨‹å›¾
- [ ] å®éªŒç»“æœå¯¹æ¯”å›¾ï¼ˆæŸ±çŠ¶å›¾ã€æŠ˜çº¿å›¾ï¼‰
- [ ] UIæˆªå›¾

**æ•°æ®ç»Ÿè®¡è¡¨**ï¼š
- [ ] å„ç³»ç»Ÿè¯„åˆ†å¯¹æ¯”è¡¨
- [ ] å“åº”æ—¶é—´ç»Ÿè®¡è¡¨
- [ ] èµ„æºå ç”¨è¡¨

#### éªŒæ”¶æ ‡å‡†
- å®Œæˆä¸¤é¡¹è¯„ä¼°å®éªŒ
- æœ‰è¯¦ç»†çš„æ•°æ®å’Œåˆ†æ
- æäº¤å®Œæ•´çš„é¡¹ç›®æŠ¥å‘Šï¼ˆPDFï¼Œ15-20é¡µï¼‰

---

## å…³é”®é‡Œç¨‹ç¢‘æ€»ç»“

| å‘¨æ¬¡ | å…³é”®äº§å‡º | å®Œæˆæ ‡å¿— |
|------|---------|---------|
| Week 1 | Rewrite + Rerank | èƒ½å¤Ÿæ”¹å†™queryå¹¶é‡æ’åºç»“æœ |
| Week 2 | æ··åˆæ£€ç´¢ + å­åº“è·¯ç”± | å®ç°åŒå‘é‡æ£€ç´¢å’Œä¸¤é˜¶æ®µæ£€ç´¢ |
| Week 3 | å…ƒæ•°æ®è¿‡æ»¤ + Deep Research Agentï¼ˆåŠ åˆ†é¡¹ï¼‰+ å®Œæ•´è¯„ä¼° | åŠŸèƒ½å®Œå¤‡ï¼Œæœ‰è¯„ä¼°æŠ¥å‘Š |

**æ ¸å¿ƒåŠŸèƒ½ä¼˜å…ˆçº§**ï¼š
1. ğŸ”´ **å¿…é¡»å®Œæˆ**ï¼šRewrite, Rerank, æ··åˆæ£€ç´¢, å­åº“è·¯ç”±ï¼ˆWeek 1-2ï¼‰
2. ğŸŸ¡ **é‡è¦åŠŸèƒ½**ï¼šå…ƒæ•°æ®è¿‡æ»¤ï¼ˆWeek 3å‰æœŸï¼‰
3. ğŸŸ¢ **åŠ åˆ†é¡¹**ï¼šDeep Research Agentï¼ˆWeek 3ä¸­æœŸï¼Œæ—¶é—´å…è®¸æ—¶ï¼‰
4. ğŸ”µ **å¿…é¡»å®Œæˆ**ï¼šè¯„ä¼°ä¸æŠ¥å‘Šï¼ˆWeek 3åæœŸï¼‰

---

## æ¯æ—¥å·¥ä½œæµç¨‹å»ºè®®

### æ¯å¤©å¼€å§‹ï¼ˆ9:00-9:30ï¼‰
- [ ] æ£€æŸ¥GPUèµ„æºå’ŒæœåŠ¡çŠ¶æ€
- [ ] å›é¡¾æ˜¨å¤©çš„è¿›å±•
- [ ] æ˜ç¡®ä»Šå¤©çš„ç›®æ ‡

### å¼€å‘æ—¶æ®µï¼ˆ9:30-12:00, 14:00-18:00ï¼‰
- [ ] é›†ä¸­å¼€å‘
- [ ] æ¯2å°æ—¶commitä¸€æ¬¡ä»£ç 
- [ ] è®°å½•å®éªŒç»“æœåˆ°ç¬”è®°

### æ¯å¤©ç»“æŸï¼ˆ18:00-18:30ï¼‰
- [ ] æ€»ç»“ä»Šå¤©çš„å·¥ä½œ
- [ ] æ›´æ–°è¿›åº¦è¡¨
- [ ] è§„åˆ’æ˜å¤©ä»»åŠ¡

### æ¯å‘¨äº”ä¸‹åˆï¼ˆ16:00-18:00ï¼‰
- [ ] å›¢é˜Ÿä¼šè®®ï¼ˆå¦‚æœ‰ï¼‰
- [ ] æ¼”ç¤ºæœ¬å‘¨æˆæœ
- [ ] è®¨è®ºä¸‹å‘¨è®¡åˆ’

---

## Deep Research Agentå®ç°å†³ç­–æŒ‡å—

### ä»€ä¹ˆæ—¶å€™å¼€å§‹å®ç°Deep Researchï¼Ÿ

**åˆ¤æ–­æ ‡å‡†**ï¼ˆæ»¡è¶³ä»¥ä¸‹æ¡ä»¶å†å¼€å§‹ï¼‰ï¼š
- âœ… Week 1-2çš„æ‰€æœ‰æ ¸å¿ƒåŠŸèƒ½éƒ½å·²å®Œæˆ
- âœ… åŸºç¡€RAG pipelineè¿è¡Œç¨³å®š
- âœ… è‡³å°‘å®Œæˆäº†ä¸€è½®åˆæ­¥æµ‹è¯•
- âœ… è¿˜å‰©è‡³å°‘2å¤©æ—¶é—´ï¼ˆDay 17-18ï¼‰

### å®ç°å“ªä¸ªç‰ˆæœ¬ï¼Ÿ

**ç®€åŒ–ç‰ˆï¼ˆæ¨èï¼‰**ï¼š
- æ—¶é—´ï¼š1.5å¤©
- å¤æ‚åº¦ï¼šä½
- åŠŸèƒ½ï¼šå›ºå®š3è½®è¿­ä»£ + åŸºç¡€UI
- ä»·å€¼ï¼šå±•ç¤ºAgentæ€ç»´å³å¯

**å®Œæ•´ç‰ˆï¼ˆæŒ‘æˆ˜ï¼‰**ï¼š
- æ—¶é—´ï¼š2å¤©
- å¤æ‚åº¦ï¼šä¸­
- åŠŸèƒ½ï¼šåŠ¨æ€ReAct + å®Œæ•´å·¥å…·é“¾
- ä»·å€¼ï¼šæ¥è¿‘OpenAI Deep Research

### å¦‚æœæ—¶é—´ä¸å¤Ÿæ€ä¹ˆåŠï¼Ÿ

**Plan Bé€‰é¡¹**ï¼š
1. **åªå†™è®¾è®¡æ–‡æ¡£**ï¼šåœ¨æŠ¥å‘Šä¸­è¯¦ç»†æè¿°å¦‚ä½•å®ç°ï¼Œä½œä¸º"æœªæ¥å·¥ä½œ"
2. **Mockæ¼”ç¤º**ï¼šç”¨é¢„è®¾çš„æ¨ç†æ­¥éª¤æ¨¡æ‹ŸAgentæ€è€ƒè¿‡ç¨‹
3. **æ”¾å¼ƒæ­¤åŠŸèƒ½**ï¼šä¸“æ³¨äºæ ¸å¿ƒåŠŸèƒ½çš„æ‰“ç£¨å’Œè¯„ä¼°

### å»ºè®®çš„æ—¶é—´åˆ†é…

```
Day 15-16: å…ƒæ•°æ®è¿‡æ»¤ï¼ˆå¿…é¡»å®Œæˆï¼‰
Day 17ä¸Šåˆ: è¯„ä¼°æ˜¯å¦å¼€å§‹Deep Research
    â”œâ”€ å¦‚æœè¿›åº¦è‰¯å¥½ â†’ å¼€å§‹å®ç°ç®€åŒ–ç‰ˆ
    â””â”€ å¦‚æœè¿›åº¦å»¶è¿Ÿ â†’ è·³è¿‡ï¼Œå¼€å§‹Day 19çš„å·¥ä½œ
Day 17ä¸‹åˆ-18: Deep Researchå®ç°ï¼ˆå¦‚æœå¼€å§‹ï¼‰
Day 19: ç³»ç»Ÿé›†æˆä¸æµ‹è¯•ï¼ˆå¿…é¡»å®Œæˆï¼‰
Day 20-21: è¯„ä¼°ä¸æŠ¥å‘Šï¼ˆå¿…é¡»å®Œæˆï¼‰
```

---

## é£é™©ç®¡ç†

### é«˜é£é™©é¡¹ï¼ˆéœ€æå‰å‡†å¤‡ï¼‰

**é£é™©1: Milvus 2.5å…¼å®¹æ€§é—®é¢˜**
- å½±å“ï¼šå¯èƒ½æ— æ³•ä½¿ç”¨æ–°ç‰¹æ€§
- åº”å¯¹ï¼šå‡†å¤‡é™çº§åˆ°2.4çš„æ–¹æ¡ˆï¼Œä½¿ç”¨åªè¯»è´¦å·å‚è€ƒ

**é£é™©2: GPUèµ„æºä¸è¶³**
- å½±å“ï¼šå‘é‡åŒ–é€Ÿåº¦æ…¢ï¼Œå¯èƒ½æ‹–å»¶è¿›åº¦
- åº”å¯¹ï¼š
  - åˆ†æ‰¹å¤„ç†æ•°æ®
  - ä½¿ç”¨CPUæ¨¡å¼ï¼ˆé€Ÿåº¦æ…¢ä½†å¯ç”¨ï¼‰
  - è°ƒæ•´batch size

**é£é™©3: LangChain APIå˜åŒ–**
- å½±å“ï¼šç°æœ‰ä»£ç å¯èƒ½ä¸å…¼å®¹
- åº”å¯¹ï¼š
  - æŸ¥é˜…æœ€æ–°æ–‡æ¡£
  - å‚è€ƒå®˜æ–¹migration guide
  - ä¿ç•™æ—§ç‰ˆæœ¬ä»£ç ä½œä¸ºå¤‡ä»½

### ä¸­é£é™©é¡¹

**é£é™©4: Deep Researchå®ç°æ—¶é—´ä¸è¶³**
- å½±å“ï¼šåŠ åˆ†é¡¹æ— æ³•å®Œæˆ
- åº”å¯¹ï¼š
  - ä¼˜å…ˆç¡®ä¿æ ¸å¿ƒåŠŸèƒ½å®Œæˆ
  - å®ç°ç®€åŒ–ç‰ˆæœ¬
  - æˆ–åªåœ¨æŠ¥å‘Šä¸­æè¿°è®¾è®¡æ–¹æ¡ˆ

**é£é™©5: LLMçš„ReActèƒ½åŠ›ä¸è¶³**
- å½±å“ï¼šAgentæ— æ³•æ­£ç¡®æ¨ç†
- åº”å¯¹ï¼š
  - ç®€åŒ–promptè®¾è®¡
  - å¢åŠ ç¤ºä¾‹ï¼ˆfew-shotï¼‰
  - é™çº§ä¸ºå›ºå®šæµç¨‹çš„multi-stepæ£€ç´¢

**é£é™©6: æ•°æ®è´¨é‡é—®é¢˜**
- å½±å“ï¼šæ£€ç´¢æ•ˆæœä¸ä½³
- åº”å¯¹ï¼šæ‰‹åŠ¨ç­›é€‰é«˜è´¨é‡è®ºæ–‡å­é›†è¿›è¡Œæµ‹è¯•

**é£é™©7: è¯„ä¼°æ ‡å‡†ä¸æ˜ç¡®**
- å½±å“ï¼šéš¾ä»¥é‡åŒ–æ”¹è¿›æ•ˆæœ
- åº”å¯¹ï¼šæå‰ä¸å¯¼å¸ˆæ²Ÿé€šè¯„ä¼°æ–¹å¼

---

## èµ„æºæ¸…å•

### å¼€å‘ç¯å¢ƒ
- GPUæœåŠ¡å™¨ï¼šï¼ˆå¾…åˆ†é…ï¼‰
- Milvusæ•°æ®åº“ï¼š10.19.48.181:19530
- å¤§æ¨¡å‹æœåŠ¡ï¼šhttp://10.15.102.186:9000

### æ–‡æ¡£ä¸å·¥å…·
- Milvuså®˜æ–¹æ–‡æ¡£ï¼šhttps://milvus.io/docs
- LangChainæ–‡æ¡£ï¼šhttps://python.langchain.com/
- BGE-M3è®ºæ–‡ï¼šhttps://arxiv.org/abs/2402.03216
- é¡¹ç›®ä»£ç ï¼šï¼ˆå·²æä¾›5ä¸ªæ–‡ä»¶ï¼‰

### æ•°æ®èµ„æº
- è®ºæ–‡é›†ä¸‹è½½ï¼šï¼ˆè§é¡¹ç›®æ–‡æ¡£ï¼‰
- æµ‹è¯•é—®é¢˜é›†ï¼šï¼ˆéœ€è‡ªè¡Œå‡†å¤‡ï¼‰

---

## é™„å½•ï¼šå¿«é€Ÿå‚è€ƒ

### å¸¸ç”¨å‘½ä»¤

```bash
# è¿æ¥Milvus
from pymilvus import connections
connections.connect(
    alias="default",
    host="10.19.48.181",
    port="19530",
    user="cs286_2025_groupX",
    password="GroupX"
)

# è°ƒç”¨Ollamaæ¨¡å‹
curl http://10.15.102.186:9000/api/generate -d '{
  "model": "qwen3:30b-a3b-instruct-2507-q8_0",
  "prompt": "What is XFEL?",
  "stream": false
}'

# æŸ¥çœ‹Milvus collection
from pymilvus import Collection
collection = Collection("your_collection_name")
print(collection.num_entities)
print(collection.schema)
```

### é‡è¦æé†’

1. **ä»£ç ç‰ˆæœ¬æ§åˆ¶**
   - æ¯å¤©è‡³å°‘commit 2æ¬¡
   - é‡è¦åŠŸèƒ½å®Œæˆåç«‹å³commit
   - å†™æ¸…æ¥šcommit message

2. **å®éªŒè®°å½•**
   - åˆ›å»ºExcel/Markdownè®°å½•æ‰€æœ‰å®éªŒ
   - è®°å½•å‚æ•°ã€ç»“æœã€è§‚å¯Ÿ
   - æˆªå›¾ä¿å­˜é‡è¦ç»“æœ

3. **å®šæœŸå¤‡ä»½**
   - ä»£ç pushåˆ°Git
   - å®éªŒæ•°æ®å®šæœŸå¤‡ä»½
   - é‡è¦æ–‡ä»¶å¤šå¤„ä¿å­˜

4. **æ—¶é—´ç®¡ç†**
   - ä¸¥æ ¼æŒ‰ç…§checkpointè¿›è¡Œ
   - é‡åˆ°å›°éš¾åŠæ—¶è°ƒæ•´
   - ä¼˜å…ˆå®Œæˆæ ¸å¿ƒåŠŸèƒ½

---

## è”ç³»æ–¹å¼ä¸æ±‚åŠ©

é‡åˆ°é—®é¢˜æ—¶çš„æ±‚åŠ©é¡ºåºï¼š

1. **æŸ¥é˜…æ–‡æ¡£**ï¼šå®˜æ–¹æ–‡æ¡£å’Œå·²æœ‰ä»£ç 
2. **æœç´¢å¼•æ“**ï¼šGitHub Issuesã€Stack Overflow
3. **å›¢é˜Ÿè®¨è®º**ï¼šä¸é˜Ÿå‹è®¨è®ºï¼ˆå¦‚æœ‰ï¼‰
4. **å‘å¯¼å¸ˆæ±‚åŠ©**ï¼šå‡†å¤‡å¥½é—®é¢˜æè¿°å’Œå·²å°è¯•çš„æ–¹æ¡ˆ

---

**ç¥é¡¹ç›®é¡ºåˆ©ï¼åŠ æ²¹ï¼** ğŸš€
