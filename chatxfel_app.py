from onnx import ModelProto
import streamlit as st
import sys
import time
from datetime import datetime
from langchain_classic.retrievers.document_compressors.cross_encoder_rerank import CrossEncoderReranker
from langchain_community.cross_encoders import HuggingFaceCrossEncoder
from langchain_classic.retrievers import ContextualCompressionRetriever
from langchain_community.chat_models import ChatOllama
from streamlit import session_state as ss
from streamlit.runtime.scriptrunner import get_script_run_ctx
sys.path.append('/home/zhangxf2/LLM/llm-shine/ChatXFEL')
import rag, utils

# App title
#st.set_page_config(page_title="ChatXFEL", layout='wide')
st.set_page_config(page_title="ChatXFEL Beta 1.0", page_icon='./draw/logo.png')

st.header('ChatXFEL: Q & A System for XFEL')
if 'agree' not in ss:
    ss['agree'] = False
def update_agree():
    ss['agree'] = True
    
if not ss['agree']:
    with st.empty():
        msg = '''This page is an intelligent system to answer the questions in the field of XFEL. If you click the **agree box** below, 
        :red[you IP and the time will be recorded]. If you don't agree with that, please close the page. 
        This note will appear again when you refresh the page.''' 
        st.markdown(msg)
    agree = st.checkbox('Agree', key='read', value=False, on_change=update_agree)
    while True:
        time.sleep(3600)

def reset_retriever_cache():
    try:
        get_retriever.clear()
        get_retriever_runtime.clear()
    except Exception as e:
        pass

with st.sidebar:
    st.title('ChatXFEL Beta 1.0')
    #st.markdown('[About ChatXFEL](https://confluence.cts.shanghaitech.edu.cn/pages/viewpage.action?pageId=129762874)')
    st.markdown('[ChatXFEL简介与提问技巧](https://confluence.cts.shanghaitech.edu.cn/pages/viewpage.action?pageId=129762874)')
    #st.write(':red[You have agreed the recording of your IP and access time.]')
    #st.markdown('**IMPORTANT: The answers given by ChatXFEL are for informational purposes only, please consult the references in the source.**')
    st.markdown('**重要提示：大模型的回答仅供参考，点击Sources查看参考文献**')
    # Refactored from https://github.com/a16z-infra/llama2-chatbot
    #st.subheader('Models and parameters')
    #model_list = ['LLaMA3.1-8B', 'Qwen2.5-7B']
    model_list = ['Qwen3-30B', 'QwQ-32B']
    col_list = ['chatxfel', 'report', 'book']
    embedding_list = ['BGE-M3']

    selected_model = st.sidebar.selectbox('LLM model', model_list, index=0, key='selected_model')
    n_recall = 6 if selected_model.startswith('Q') else 5
    #if selected_model == 'LLaMA3-8B':
    #    #model_path = '/data-10gb/data/llm/gguf/Meta-Llama-3-8B-Instruct-Q8_0.gguf'
    #    n_recall = 5
    #elif selected_model == 'LLaMA3.1-8B':
    #    #model_path = '/data-10gb/data/llm/gguf/Meta-Llama-3-8B-Instruct-Q8_0.gguf'
    #    n_recall = 5
    #elif selected_model == 'Qwen2.5-7B':
    #    #model_path = '/data'
    #    n_recall = 5
    #elif selected_model == 'Qwen2.5-14B':
    #    #model_path = '/data-10gb/data/llm/qwen/qwen2-7b-instruct-q8_0.gguf'
    #    n_recall = 5

    selected_em = st.sidebar.selectbox('Embedding model', embedding_list, key='selected_em')
    if selected_em == 'llama2-7b':
        col_list.append('llama2_7b')
    elif selected_em == 'llama3-8b':
        col_list.append('llama3_8b')
    selected_col = st.sidebar.selectbox('Bibliography collection', col_list, key='select_col', on_change=reset_retriever_cache)
    col_name = selected_col
    with st.popover('About the collection'):
        if col_name == 'book':
            msg = '''This collection now only contains some theses from EuXFEL.'''
            st.markdown(msg)
        if col_name == 'chatxfel':
            msg = '''This collection contains 3000+ publications of wordwide XFEL facilities, so ChatXFEL may be slower than other collections'''
            st.markdown(msg)
        if col_name == 'report':
            msg = '''This collection only contains unpulished references, e.g CDR, TDR, engineering reports.'''
            st.markdown(msg)

    filter_year = st.sidebar.checkbox('Filter papers by year', key='filter_year', value=True)
    if filter_year:
        min_year = 1949
        max_year = datetime.now().year
        year1, year2 = st.columns([1,1])
        #year_start = year1.selectbox('Start', list(range(min_year, max_year+1))[::-1], key='year_start', index=max_year-min_year)
        year_start = year1.selectbox('Start', list(range(min_year, max_year+1))[::-1], key='year_start', index=max_year-2000)
        year_end = year2.selectbox('End', list(range(year_start, max_year+1))[::-1], key='year_end')
    filter_title = False
    #filter_title = st.sidebar.checkbox('Filter by keywords', key='filter_title', value=False)
    #if filter_title:
    #    keywords = []
    #    key_title = st.sidebar.text_input('Keywords in title', key='key_title', placeholder='seperate by comma')
    #    if key_title != '':
    #        words = key_title.split(',')
    #        for word in words:
    #            keywords.append(word.strip().lower())

    n_batch, n_ctx, max_tokens = 512, 8192, 8192
    #return_source = st.sidebar.checkbox('Return Source', key='source', value=True)
    return_source = True
    use_mongo = True
    enable_chat_history = st.sidebar.checkbox('Enable chat history', key='chat_history', value=True)
    if enable_chat_history:
        with st.popover(':information_source: About chat history'):
            msg = '''When enabled, the model will see previous conversation turns to provide context-aware responses.
            This helps with follow-up questions and references to previous answers.'''
            st.markdown(msg)
    enable_log = st.sidebar.checkbox('Enable log', key='log', value=True)
    use_monog = False
    if enable_log:
        with st.popover(':warning: :red[About the log]'):
            msg = '''All the questions, answers, retrieved documents, and the question time will be logged.
            The logs would only be used for the development of ChatXFEL. \n\nIf you don't like the log, just uncheck the box "Enable log" above.
            \n\n**Your IP address will always be recorded.**'''
            st.markdown(msg)

@st.cache_resource
def get_embedding(embedding_model, n_ctx, n_gpu_layers=1):
    print(f"{time.strftime('%Y-%m-%d %H:%M:%S')}: getting embedding...")
    # Get embedding
    if embedding_model == 'BGE-M3':
        embedding = rag.get_embedding_bge()
    return embedding
embedding = get_embedding(embedding_model=selected_em, n_ctx=n_ctx)
#print(f'Embedding: {embedding}')

#@st.cache_resource
#def get_llm(model_name, model_path, n_batch, n_ctx, max_tokens):
#    print(f"{time.strftime('%Y-%m-%d %H:%M:%S')}: getting llm...")
#    # Get llm
#    llm = rag.get_llm_llama(model_name=model_name, model_path=model_path, n_batch=n_batch,n_ctx=n_ctx,verbose=False,
#                            streaming=True,max_tokens=max_tokens, temperature=0.8)
#    return llm
#llm = get_llm(selected_model, model_path, n_batch, n_ctx, max_tokens)

#def get_llm_ollama(model_name, num_predict, num_ctx=8192, keep_alive=-1, temperature=0.1, base_url='http://10.15.85.78:11434'):
@st.cache_resource
def get_llm(model_name, num_predict, keep_alive, num_ctx=8192, temperature=0.0):
    print(f"{time.strftime('%Y-%m-%d %H:%M:%S')}: getting LLM...")
    llm = rag.get_llm_ollama(model_name=model_name, num_predict=num_predict, 
                             keep_alive=keep_alive, num_ctx=num_ctx, temperature=temperature, base_url='http://10.15.102.186:9000')
    return llm
llm = get_llm(model_name=selected_model, num_predict=2048, keep_alive=-1)

#You should answer the question in detail as far as possible. Do not make up questions by yourself.
#If you cannot find anwser in the context, just say that you don't know, don't try to make up an answer.
#Please remember some common abbrevations: SFX is short for serial femtosecond crystallography, SPI is
#short for single particle imaging.
#
#{context}
#
#Question: {question}
#Helpful Answer:"""

# Load appropriate prompt template based on chat history setting
if enable_chat_history:
    prompt_file = 'prompts/chat_with_history.pt'
else:
    prompt_file = 'prompts/naive.pt'

with open(prompt_file, 'r') as f:
    prompt_template = f.read()

@st.cache_data
def get_prompt_template(template):
    print(f"{time.strftime('%Y-%m-%d %H:%M:%S')}: getting prompt...")
    prompt = rag.get_prompt(template)
    return prompt
prompt = get_prompt_template(template=prompt_template)

@st.cache_resource
def get_rerank_model(model_name='', top_n=n_recall):
    print(f"{time.strftime('%Y-%m-%d %H:%M:%S')}: getting rerank model...")
    if model_name == '':
        model_name = 'BAAI/bge-reranker-v2-m3'
    rerank_model = HuggingFaceCrossEncoder(model_name=model_name)
    compressor = CrossEncoderReranker(model=rerank_model, top_n=top_n)
    return compressor

connection_args = utils.get_milvus_connection(
    host='10.19.48.181',
    port=19530,
    username='cs286_2025_group8',
    password='Group8',
    db_name='cs286_2025_group8'
)
@st.cache_resource
def get_retriever(connection_args, col_name, _embedding):
    print(f"{time.strftime('%Y-%m-%d %H:%M:%S')}: getting retriever...")
    if selected_em in ['llama2-7b', 'llama3-8b']:
        retriever = rag.get_retriever(connection_args=connection_args, col_name=col_name,
                                      embedding=_embedding, use_rerank=False, return_as_retreiever=False)
    else:
        retriever = rag.get_retriever(connection_args=connection_args, col_name=col_name,
                                      embedding=_embedding, vector_field='dense_vector',
                                      use_rerank=False, return_as_retreiever=False)
    return retriever

@st.cache_resource
def get_retriever_runtime(_retriever_obj, _compressor, filters=None):
    #print(f"{time.strftime('%Y-%m-%d %H:%M:%S')}: getting retriever at runtime...")
    #search_kwargs = {'k':10, 'params': {'ef': 20}}
    search_kwargs = {'k':10}
    if filters:
        search_kwargs = {**search_kwargs, **filters}
    compression_retriever = ContextualCompressionRetriever(base_compressor=_compressor,
                                                   base_retriever=_retriever_obj.as_retriever(search_kwargs=search_kwargs))
    return compression_retriever

retriever_obj = get_retriever(connection_args, selected_col, embedding)
compressor = get_rerank_model(top_n=n_recall)
filters = {}
if filter_year:
    #filters['expr'] = f'year >= {year_start} and year <= {year_end}'
    filters['expr'] = f'{year_start} <= year <= {year_end}'
#if filter_title:
#    expr_title = ''
#    for i, word in enumerate(keywords):
#        if i ==  len(keywords) -1:
#            expr_title += f'\"{word}\" in title'
#        else:
#            expr_title += f'\"{word}\" in title and '
#    if 'expr' in filters.keys():
#        filters['expr'] += ' and ' + expr_title
#    else:
#        filters['expr'] = expr_title
    
retriever = get_retriever_runtime(retriever_obj, compressor, filters=filters)

initial_message = {"role": "assistant", "content": "What do you want to know about XFEL?"}
# Store LLM generated responses
if "messages" not in ss.keys():
    ss.messages = [initial_message]

def log_feedback(feedback:dict, use_mongo):
    if feedback.get('Feedback', '') == '':
        feedback['Feedback'] = ss['feedback']+1
    utils.log_rag(feedback, use_mongo=use_mongo)

for message in ss.messages:
    with st.chat_message(message["role"]):
        st.write(message["content"])
        #try:
        c = st.columns([8,2.5])
        if 'source' in message.keys():
            with c[0].popover('Sources'):
                st.markdown(message['source'])
            if message == ss.messages[-1]:
                if 'feedback' in ss:
                    ss['feedback'] = None
                with c[1]:
                    feedback = st.feedback('stars', key='feedback', on_change=log_feedback, args=({'Feedback':''}, use_mongo,))
                    #if feedback is not None:
                    #    log_feedback({'Feedback':str(feedback+1)}, use_mongo=use_mongo)
                #with c[1]:
                #    good = st.button(':thumbsup:', key='feedback_good_1', on_click=log_feedback, args=({'Feedback':'Good'},use_mongo,))
                #with c[2]:
                #    bad = st.button(':thumbsdown:', key='feedback_bad_1', on_click=log_feedback, args=({'Feedback':'Bad'}, use_mongo,))
        #except Exception as e:
        #    pass
        #num += 1
        #if 'messages' in ss.keys():
        #    ele = st.columns([3,1,1,1,1])
        #    if 'source' in message.keys():
        #        with ele[0]:
        #            with st.popover('Show source'):
        #                st.write(message['source'])
        #        if message['role'] == 'assistant':
        #            ele[1].button('Like', key=f'01{num}')
        #            ele[2].button('Dislike', key=f'02{num}')
        #            ele[3].button('Retry', key=f'03{num}')
        #            ele[4].button('Modify', key=f'04{num}')

def clear_chat_history():
    #ss.messages = [{"role": "assistant", "content": "How may I assist you today?"}]
    ss.messages = [initial_message]

st.sidebar.button('Clear Chat History', on_click=clear_chat_history)

# Function for generating LLaMA2 response
def generate_llama2_response(question, use_history=False):
    # Format chat history if enabled
    history_text = ""
    if use_history and len(ss.messages) > 1:
        # Build chat history from previous messages (excluding initial message and current question)
        for dict_message in ss.messages[1:-1]:  # Skip initial greeting and current user question
            if dict_message["role"] == "user":
                history_text += "User: " + dict_message["content"] + "\n"
            elif dict_message["role"] == "assistant":
                # Get content without source references
                content = dict_message["content"]
                history_text += "Assistant: " + content + "\n"

        if history_text:
            history_text = history_text.strip()
        else:
            history_text = "No previous conversation."
    else:
        history_text = "No previous conversation."

    # Call RAG with or without history
    if use_history:
        output = rag.retrieve_generate(
            question=question,
            llm=llm,
            prompt=prompt,
            retriever=retriever,
            history=history_text,
            return_source=return_source,
            return_chain=False
        )
    else:
        output = rag.retrieve_generate(
            question=question,
            llm=llm,
            prompt=prompt,
            retriever=retriever,
            return_source=return_source,
            return_chain=False
        )

    return output

@st.cache_data
def log_ip_time(session_id):
    ip = session.request.remote_ip
    print(f"{time.strftime('%Y-%m-%d %H:%M:%S')}: {ip} connected or refreshed!", flush=True)

ctx = get_script_run_ctx()
client_ip = ''
if ctx:
    session = st.runtime.get_instance().get_client(ctx.session_id)
    client_ip = session.request.remote_ip
    log_ip_time(ctx.session_id)

# User-provided prompt
question_time = ''
if question:= st.chat_input():
    if enable_log:
        question_time = time.strftime('%Y-%m-%d %H:%M:%S')
    ss.messages.append({"role": "user", "content": question})
    with st.chat_message("user"):
        st.write(question)

# Generate a new response if last message is not from assistant
if 'feedback_good' not in ss:
    ss['feedback_good'] = None
if 'feedback_bad' not in ss:
    ss['feedback_bad'] = None

if ss.messages[-1]["role"] != "assistant":
    with st.chat_message("assistant"):
        with st.spinner("Thinking..."):
            p = ' Please answer the question as detailed as possible and make up you answer in markdown format.'
            response = generate_llama2_response(f'{question}{p}', use_history=enable_chat_history)
            #response = generate_llama2_response(question)
            placeholder = st.empty()
            full_response = ''
            source = ''
            if return_source:
                full_response += response['answer']
                placeholder.markdown(full_response)
                #full_response += '\nContext: \n'
                for i, c in enumerate(response['context']):
                    source += f'{c.page_content}'
                    title = c.metadata.get('title') if 'title' in c.metadata.keys() else c.metadata.get('source')
                    doi = c.metadata.get('doi', '')
                    journal = c.metadata.get('journal', '')
                    year = c.metadata.get('year', '')
                    page = c.metadata.get('page')
                    if doi == '':
                        source += f'\n\n**Ref. {i+1}**: {title}, {journal}, {year}, page {page}'
                    else:
                        source += f'\n\n**Ref. {i+1}**: {title}, {journal}, {year}, [{doi}](http://dx.doi.org/{doi}), page {page}'
                    if i != len(response['context'])-1:
                        source += '\n\n'
                    #placeholder.markdown(source)
                    if i == len(response['context'])-1:
                        c = st.columns([8,3])
                        with c[0].popover('Source'):
                            st.markdown(source)
                        #with c[1]:
                        #    feedback = st.feedback('stars', key='feedback')
                        #    if feedback is not None:
                        #        log_feedback({'Feedback':str(feedback+1)}, use_mongo=use_mongo)
                        #with c[1]:
                        #    good = st.button(':thumbsup:', key='feedback_good', on_click=log_feedback, args=({'Feedback':'Good'},use_mongo,))
                        #with c[2]:
                        #    bad = st.button(':thumbsdown:', key='feedback_bad', on_click=log_feedback, args=({'Feedback':'Bad'}, use_mongo,))
            else:
                full_response = response.content
                #for item in response:
                #    full_response += item
                #    placeholder.markdown(full_response)
                placeholder.markdown(full_response)
            #placeholder.markdown(full_response)

    if return_source:
        message = {"role": "assistant", "content": full_response, "source":source}
        #if enable_log:
            #utils.log_rag(client_ip, question_time, question, full_response, source, use_mongo=False)
    else:
        message = {"role": "assistant", "content": full_response}
        #if enable_log:
        #    utils.log_rag(client_ip, question_time, question, full_response, use_mongo=False)
    if enable_log:
        logs = {'IP':client_ip, 'Time':question_time, 'Model':selected_model, 'Question': question, 'Answer':full_response, 'Source':source}
        utils.log_rag(logs, use_mongo=use_mongo)
    ss.messages.append(message)
    st.rerun()
#c = st.columns([8,2.5])
#feedback = st.feedback('stars', key='feedback')
#with c[0]:
#    pass
#with c[1]:
#    if feedback is not None:
#        log_feedback({'Feedback':str(feedback+1)}, use_mongo=use_mongo)
    #history.append({'role':'user','content':question})
    #history.append({'role':'assistant', 'content':full_response})
