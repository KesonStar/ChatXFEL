# åˆ›å»ºçš„æ–‡ä»¶æ¸…å•

æœ¬æ¬¡ä¸ºXFELBenchåˆ›å»ºçš„å®Œæ•´è¯„ä¼°pipelineåŒ…å«ä»¥ä¸‹æ–‡ä»¶ï¼š

## æ ¸å¿ƒè„šæœ¬

### 1. `llm_judge.py` â­
**åŠŸèƒ½**: LLMè¯„åˆ¤å™¨ï¼Œä½¿ç”¨GPT-4o-miniå¯¹RAGç­”æ¡ˆè¿›è¡Œä¸‰ç»´åº¦è¯„åˆ†

**ç”¨æ³•**:
```bash
python llm_judge.py \
    --results outputs/20251230_230056_baseline/results.jsonl \
    --output evaluations/baseline_eval \
    --problem-set problem_sets/problem_set.md
```

**è¯„ä¼°ç»´åº¦**:
- Factual Accuracy (1-5åˆ†)
- Groundedness / Evidence Use (1-5åˆ†)
- Coverage & Specificity (1-5åˆ†)

**è¾“å‡º**:
- `evaluation_results.jsonl` - æ¯ä¸ªé—®é¢˜çš„è¯¦ç»†è¯„åˆ†
- `evaluation_summary.json` - æ±‡æ€»ç»Ÿè®¡

---

### 2. `generate_configs.py` â­
**åŠŸèƒ½**: è‡ªåŠ¨ç”Ÿæˆå¤šä¸ªå®éªŒé…ç½®æ–‡ä»¶

**ç”¨æ³•**:
```bash
# ç”Ÿæˆæ‰€æœ‰12ä¸ªé…ç½®
python generate_configs.py

# ç”Ÿæˆç‰¹å®šé…ç½®
python generate_configs.py --configs baseline hybrid_search

# åˆ—å‡ºå¯ç”¨é…ç½®
python generate_configs.py --list
```

**ç”Ÿæˆçš„é…ç½®**:
- `baseline` - åŸºçº¿é…ç½®
- `no_rerank` - æ— é‡æ’åº
- `hybrid_search` - æ··åˆæœç´¢
- `hybrid_dense_heavy` - Denseåé‡
- `hybrid_sparse_heavy` - Sparseåé‡
- `query_rewrite` - æŸ¥è¯¢é‡å†™
- `routing` - è·¯ç”±æ£€ç´¢
- `hybrid_rewrite` - æ··åˆ+é‡å†™
- `hybrid_routing` - æ··åˆ+è·¯ç”±
- `full_features` - å…¨åŠŸèƒ½
- `rerank_top3` - é‡æ’Top3
- `rerank_top10` - é‡æ’Top10

**è¾“å‡ºä½ç½®**: `configs/generated/`

---

### 3. `run_full_evaluation.py` â­â­â­
**åŠŸèƒ½**: ä¸»æ§è„šæœ¬ï¼Œorchestrateå®Œæ•´çš„è¯„ä¼°pipeline

**æµç¨‹**:
1. ç”Ÿæˆé…ç½®æ–‡ä»¶
2. å¯¹æ¯ä¸ªé…ç½®è¿è¡ŒRAGè¯„ä¼°
3. å¯¹æ¯ä¸ªç»“æœè¿è¡ŒLLMè¯„åˆ¤
4. ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š

**ç”¨æ³•**:
```bash
# è¿è¡Œæ‰€æœ‰é…ç½®
python run_full_evaluation.py --questions problem_sets/xfel_qa_basic.json

# è¿è¡Œç‰¹å®šé…ç½®
python run_full_evaluation.py \
    --questions problem_sets/xfel_qa_basic.json \
    --configs baseline hybrid_search full_features

# è·³è¿‡LLMè¯„åˆ¤ï¼ˆä»…ç”Ÿæˆç­”æ¡ˆï¼‰
python run_full_evaluation.py \
    --questions problem_sets/xfel_qa_basic.json \
    --skip-llm-judge
```

**è¾“å‡º**:
- RAGç»“æœ: `outputs/TIMESTAMP_CONFIG/`
- è¯„ä¼°ç»“æœ: `evaluations/TIMESTAMP_CONFIG/`
- æ€»ç»“æŠ¥å‘Š: `evaluations/summary_TIMESTAMP/`

---

### 4. `compare_results.py`
**åŠŸèƒ½**: æ¯”è¾ƒå’Œå¯è§†åŒ–ä¸åŒé…ç½®çš„è¯„ä¼°ç»“æœ

**ç”¨æ³•**:
```bash
# æ˜¾ç¤ºæ’åè¡¨æ ¼
python compare_results.py

# ç”ŸæˆCSVæŠ¥å‘Š
python compare_results.py --csv comparison.csv

# å¯¹æ¯”ç‰¹å®šé…ç½®
python compare_results.py --compare baseline hybrid_search full_features

# æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
python compare_results.py --stats
```

**åŠŸèƒ½ç‰¹æ€§**:
- æ’åè¡¨æ ¼
- å„ç»´åº¦æœ€ä½³é…ç½®
- åˆ†æ•°åˆ†å¸ƒç»Ÿè®¡
- è¯¦ç»†å¯¹æ¯”åˆ†æ
- CSVå¯¼å‡º

---

## Shellè„šæœ¬

### 5. `run_all.sh` â­â­â­
**åŠŸèƒ½**: ä¸€é”®è¿è¡Œè„šæœ¬ï¼Œæœ€ç®€å•çš„ä½¿ç”¨æ–¹å¼

**ç”¨æ³•**:
```bash
# è®¾ç½®APIå¯†é’¥
export OPENAI_API_KEY="your-key"

# è¿è¡Œ
./run_all.sh
```

**å‚æ•°** (å¯é€‰):
```bash
./run_all.sh [QUESTION_FILE] [PROBLEM_SET] [CONFIGS]
```

---

### 6. `quick_test.sh`
**åŠŸèƒ½**: å¿«é€Ÿæµ‹è¯•è„šæœ¬ï¼ŒéªŒè¯pipelineè®¾ç½®

**ç”¨æ³•**:
```bash
./quick_test.sh
```

**æµ‹è¯•å†…å®¹**:
- ç”Ÿæˆ2ä¸ªé…ç½®ï¼ˆbaseline, hybrid_searchï¼‰
- è¿è¡ŒRAGè¯„ä¼°
- è¿è¡ŒLLMè¯„åˆ¤ï¼ˆå¦‚æœAPI keyå¯ç”¨ï¼‰
- æ˜¾ç¤ºå¯¹æ¯”ç»“æœ

---

### 7. `example_evaluation.sh`
**åŠŸèƒ½**: LLM judgeçš„ç¤ºä¾‹è„šæœ¬

**ç”¨æ³•**:
```bash
# ä¿®æ”¹è„šæœ¬ä¸­çš„OPENAI_API_KEY
./example_evaluation.sh
```

---

## æ–‡æ¡£

### 8. `FULL_PIPELINE_README.md` â­â­
**å†…å®¹**: å®Œæ•´pipelineçš„è¯¦ç»†ä½¿ç”¨æ–‡æ¡£

**åŒ…å«**:
- å¿«é€Ÿå¼€å§‹æŒ‡å—
- æ‰€æœ‰é…ç½®è¯´æ˜
- è¯¦ç»†ç”¨æ³•ç¤ºä¾‹
- æ•…éšœæ’é™¤
- æœ€ä½³å®è·µ
- APIæˆæœ¬ä¼°ç®—

---

### 9. `LLM_JUDGE_README.md`
**å†…å®¹**: LLMè¯„åˆ¤å™¨çš„è¯¦ç»†æ–‡æ¡£

**åŒ…å«**:
- è¯„ä¼°ç»´åº¦è¯´æ˜
- è¯„åˆ†æ ‡å‡†
- ä½¿ç”¨ç¤ºä¾‹
- Ground truthå¤„ç†
- APIé…ç½®
- ç¼–ç¨‹æ¥å£

---

### 10. `FILES_CREATED.md` (æœ¬æ–‡ä»¶)
**å†…å®¹**: æ‰€æœ‰åˆ›å»ºæ–‡ä»¶çš„æ¸…å•å’Œè¯´æ˜

---

## ä½¿ç”¨æµç¨‹

### æ–¹æ¡ˆA: ä¸€é”®è¿è¡Œï¼ˆæ¨èåˆæ¬¡ä½¿ç”¨ï¼‰

```bash
# 1. è®¾ç½®APIå¯†é’¥
export OPENAI_API_KEY="sk-xxx"

# 2. è¿è¡Œ
./run_all.sh

# 3. æŸ¥çœ‹ç»“æœ
python compare_results.py
```

### æ–¹æ¡ˆB: åˆ†æ­¥è¿è¡Œï¼ˆæ¨èè°ƒè¯•å’Œè‡ªå®šä¹‰ï¼‰

```bash
# 1. ç”Ÿæˆé…ç½®
python generate_configs.py

# 2. è¿è¡Œå®Œæ•´è¯„ä¼°
python run_full_evaluation.py --questions problem_sets/xfel_qa_basic.json

# 3. æ¯”è¾ƒç»“æœ
python compare_results.py --csv results.csv
```

### æ–¹æ¡ˆC: è‡ªå®šä¹‰é…ç½®

```bash
# 1. ä¿®æ”¹generate_configs.pyï¼Œæ·»åŠ è‡ªå®šä¹‰é…ç½®

# 2. ç”Ÿæˆè¯¥é…ç½®
python generate_configs.py --configs my_custom_config

# 3. è¿è¡Œè¯„ä¼°
python run_full_evaluation.py \
    --questions problem_sets/xfel_qa_basic.json \
    --configs my_custom_config

# 4. ä¸baselineå¯¹æ¯”
python compare_results.py --compare baseline my_custom_config
```

### æ–¹æ¡ˆD: ä»…è¿è¡ŒLLMè¯„åˆ¤

å¦‚æœå·²ç»æœ‰RAGç»“æœï¼š

```bash
python llm_judge.py \
    --results outputs/20251230_230056_baseline/results.jsonl \
    --output evaluations/my_eval \
    --problem-set problem_sets/problem_set.md
```

---

## å¿«é€Ÿå‚è€ƒ

### æŸ¥çœ‹å¸®åŠ©

```bash
python run_full_evaluation.py --help
python generate_configs.py --help
python llm_judge.py --help
python compare_results.py --help
```

### åˆ—å‡ºé…ç½®

```bash
python run_full_evaluation.py --list-configs
python generate_configs.py --list
```

### æŸ¥çœ‹ç»“æœ

```bash
# æœ€æ–°çš„è¯„ä¼°æŠ¥å‘Š
ls -t evaluations/summary_*/EVALUATION_REPORT.md | head -1

# æŸ¥çœ‹æŠ¥å‘Š
cat $(ls -t evaluations/summary_*/EVALUATION_REPORT.md | head -1)
```

### æ¸…ç†

```bash
# æ¸…ç†ç”Ÿæˆçš„é…ç½®
rm -rf configs/generated/

# æ¸…ç†è¾“å‡ºï¼ˆè°¨æ…ï¼ï¼‰
rm -rf outputs/
rm -rf evaluations/
```

---

## æ–‡ä»¶ä¾èµ–å…³ç³»

```
run_all.sh
    â†“
run_full_evaluation.py
    â†“
    â”œâ”€â†’ generate_configs.py â†’ configs/generated/*.yaml
    â”‚
    â”œâ”€â†’ eval_generator.py â†’ outputs/*/results.jsonl
    â”‚
    â””â”€â†’ llm_judge.py â†’ evaluations/*/evaluation_results.jsonl
            â†“
        compare_results.py
```

---

## å…³é”®ç‰¹æ€§

### âœ… å·²å®ç°

- [x] è‡ªåŠ¨ç”Ÿæˆ12ä¸ªé¢„å®šä¹‰é…ç½®
- [x] æ‰¹é‡RAGè¯„ä¼°
- [x] ä¸‰ç»´åº¦LLMè¯„åˆ¤
- [x] æ¡ä»¶æ€§ground truthå¤„ç†
- [x] ç»“æœå¯¹æ¯”å’Œæ’å
- [x] MarkdownæŠ¥å‘Šç”Ÿæˆ
- [x] CSVå¯¼å‡º
- [x] é”™è¯¯å¤„ç†å’Œé‡è¯•
- [x] è¿›åº¦è·Ÿè¸ª
- [x] å®Œæ•´æ–‡æ¡£

### ğŸ”„ å¯æ‰©å±•

- [ ] å¹¶è¡Œå¤„ç†å¤šä¸ªé…ç½®
- [ ] æ›´å¤šè¯„ä¼°ç»´åº¦
- [ ] å¯è§†åŒ–å›¾è¡¨
- [ ] ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ
- [ ] å®æ—¶è¿›åº¦ç›‘æ§
- [ ] ç»“æœç¼“å­˜

---

## é¢„æœŸè¾“å‡ºç¤ºä¾‹

### 1. é…ç½®æ’åè¡¨æ ¼

```
====================================================================================================
Configuration Ranking (by Overall Score)
====================================================================================================
Rank   Config                         Overall    Factual    Grounded   Coverage   Total Q
----------------------------------------------------------------------------------------
1      full_features                  4.23       4.35       4.18       4.15       50
2      hybrid_rewrite                 4.18       4.30       4.12       4.12       50
3      hybrid_search                  4.05       4.15       3.98       4.02       50
4      baseline                       3.95       4.05       3.88       3.92       50
...
```

### 2. æœ€ä½³é…ç½®

```
Best Configurations by Dimension
====================================================================================================
Dimension                 Configuration                  Score
-----------------------------------------------------------------
Overall                   full_features                  4.23
Factual Accuracy          full_features                  4.35
Groundedness              hybrid_routing                 4.20
Coverage & Specificity    query_rewrite                  4.18
```

### 3. è¯„ä¼°æŠ¥å‘Š

è‡ªåŠ¨ç”Ÿæˆçš„markdownæŠ¥å‘ŠåŒ…å«ï¼š
- å®Œæ•´æ’åè¡¨æ ¼
- å„ç»´åº¦åˆ†æ
- é…ç½®è¯¦æƒ…
- æ–¹æ³•è®ºè¯´æ˜

---

## æŠ€æœ¯æ ˆ

- **Python 3.10+**
- **OpenAI API** (GPT-4o-mini)
- **YAML** é…ç½®æ–‡ä»¶
- **JSONL** ç»“æœå­˜å‚¨
- **Bash** è„šæœ¬
- **Markdown** æŠ¥å‘Š

---

## æˆæœ¬ä¼°ç®—

### APIè°ƒç”¨

- æ¯ä¸ªé—®é¢˜ï¼š3æ¬¡è°ƒç”¨ï¼ˆä¸‰ä¸ªç»´åº¦ï¼‰
- 50ä¸ªé—®é¢˜ï¼š150æ¬¡è°ƒç”¨
- 12ä¸ªé…ç½®ï¼š1800æ¬¡è°ƒç”¨

### OpenAIæˆæœ¬

ä½¿ç”¨GPT-4o-mini:
- è¾“å…¥: ~$0.15/1M tokens
- è¾“å‡º: ~$0.60/1M tokens
- **ä¼°è®¡æ€»æˆæœ¬**: $2-5ï¼ˆå–å†³äºç­”æ¡ˆé•¿åº¦ï¼‰

---

## æ”¯æŒä¸åé¦ˆ

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼š

1. æŸ¥çœ‹ `FULL_PIPELINE_README.md`
2. æŸ¥çœ‹ `LLM_JUDGE_README.md`
3. è¿è¡Œ `quick_test.sh` è¯Šæ–­é—®é¢˜
4. è”ç³»å¼€å‘å›¢é˜Ÿ

---

## ç‰ˆæœ¬ä¿¡æ¯

- **åˆ›å»ºæ—¥æœŸ**: 2025-01-02
- **ç‰ˆæœ¬**: 1.0
- **å…¼å®¹æ€§**: XFELBench 1.0+

---

## æ€»ç»“

æœ¬è¯„ä¼°pipelineæä¾›äº†ï¼š

1. **è‡ªåŠ¨åŒ–**: ä¸€é”®è¿è¡Œå®Œæ•´è¯„ä¼°
2. **çµæ´»æ€§**: 12ä¸ªé¢„å®šä¹‰é…ç½® + è‡ªå®šä¹‰é€‰é¡¹
3. **å…¨é¢æ€§**: ä¸‰ç»´åº¦è¯„åˆ† + è¯¦ç»†åˆ†æ
4. **å¯ç”¨æ€§**: æ¸…æ™°çš„æ–‡æ¡£å’Œç¤ºä¾‹
5. **å¯æ‰©å±•æ€§**: æ˜“äºæ·»åŠ æ–°é…ç½®å’Œè¯„ä¼°ç»´åº¦

**æ¨èå…¥é—¨æ–¹å¼**: å…ˆè¿è¡Œ `./quick_test.sh` æµ‹è¯•è®¾ç½®ï¼Œç„¶åè¿è¡Œ `./run_all.sh` å®Œæ•´è¯„ä¼°ã€‚
